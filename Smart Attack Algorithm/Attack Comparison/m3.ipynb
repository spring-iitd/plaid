{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 10:00:38.818905: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fuzzy_dataset.csv', 'normal_run_data.7z', 'normal_run_data', 'DoS_dataset.csv', 'RPM_dataset.csv', 'gear_dataset.csv']\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'Car-Hacking/'\n",
    "print(os.listdir(data_folder))\n",
    "\n",
    "smart_attack_path = 'attack_10_10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_data_path = os.path.join(data_folder, 'RPM_dataset.csv')\n",
    "gear_data_path = os.path.join(data_folder, 'gear_dataset.csv')\n",
    "dos_data_path = os.path.join(data_folder, 'DoS_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_bin(hex_num):\n",
    "    \n",
    "    binary_value = bin(int(str(hex_num), 16))[2:]\n",
    "    \n",
    "    return binary_value\n",
    "\n",
    "def int_to_bin(int_num):\n",
    "    \n",
    "    binary_value = bin(int_num)[2:]\n",
    "    \n",
    "    return binary_value\n",
    "\n",
    "def pad(value, length):\n",
    "    \n",
    "    curr_length = len(str(value))\n",
    "    \n",
    "    zeros = '0' * (length - curr_length)\n",
    "    \n",
    "    return zeros + value\n",
    "\n",
    "hex_to_dec = lambda x: int(x, 16)\n",
    "\n",
    "def transform_data(data):\n",
    "\n",
    "    data['ID'] = data['ID'].apply(hex_to_dec)\n",
    "    data['Payload'] = data['Payload'].apply(hex_to_dec)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_columns(df):\n",
    "    \n",
    "    for dlc in [2,5,6]:\n",
    "\n",
    "        df.loc[df['dlc'] == dlc, df.columns[3:]] = df.loc[df['dlc'] == dlc, df.columns[3:]].shift(periods=8-dlc, axis='columns', fill_value='00')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_attack_data(data_path):\n",
    "    \n",
    "    columns = ['timestamp','can_id', 'dlc', 'data0', 'data1', 'data2', 'data3', 'data4', \n",
    "           'data5', 'data6', 'data7', 'flag']\n",
    "    \n",
    "    data = pd.read_csv(data_path, names = columns)\n",
    "\n",
    "    data = shift_columns(data)\n",
    "    \n",
    "    ##Replacing all NaNs with '00' \n",
    "    data = data.replace(np.NaN, '00')\n",
    "    \n",
    "    ##Joining all data columns to put all data in one column\n",
    "    data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "    ##The data column is in hexadecimal\n",
    "    data['data'] = data[data_cols].apply(''.join, axis=1)\n",
    "    data.drop(columns = data_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    ##Converting columns to decimal\n",
    "    data['can_id'] = data['can_id'].apply(hex_to_dec)\n",
    "    data['data'] = data['data'].apply(hex_to_dec)\n",
    "\n",
    "    data = data.assign(IAT=data['timestamp'].diff().fillna(0))\n",
    "    \n",
    "    return data[:50_000]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpm_data = read_attack_data(rpm_data_path)\n",
    "gear_data = read_attack_data(gear_data_path)\n",
    "dos_data = read_attack_data(dos_data_path)\n",
    "smart_attack = pd.read_csv(smart_attack_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3382276/3946342637.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  gear_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
      "/tmp/ipykernel_3382276/3946342637.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gear_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
      "/tmp/ipykernel_3382276/3946342637.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dos_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
      "/tmp/ipykernel_3382276/3946342637.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dos_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
      "/tmp/ipykernel_3382276/3946342637.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  rpm_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
      "/tmp/ipykernel_3382276/3946342637.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  rpm_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "gear_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
    "dos_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
    "rpm_data['flag'].replace({'R' : 0, 'T' : 1}, inplace = True)\n",
    "\n",
    "impersonation_data = pd.concat([gear_data,rpm_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencify_data(X, y, seq_size=10):\n",
    "    max_index = len(X) - seq_size + 1\n",
    "\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for i in range(0, max_index, seq_size):\n",
    "        X_seq.append(X[i:i+seq_size])  # Append the sequence from DataFrame 'X'\n",
    "        try:\n",
    "            y_seq.append(1 if 1 in y[i:i+seq_size].values else 0)  # Check for '1' in 'y' values\n",
    "        except:\n",
    "             y_seq.append(1 if 1 in y[i:i+seq_size] else 0)\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_data.drop(columns = ['timestamp'], inplace = True)\n",
    "impersonation_data.drop(columns = ['timestamp'], inplace = True)\n",
    "smart_attack.drop(columns = ['Timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOS: flag\n",
      "0    38580\n",
      "1    11420\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Impersonation Combined: flag\n",
      "0    81402\n",
      "1    18598\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Smart: label\n",
      "0.0    100000\n",
      "2.0     29271\n",
      "1.0     18191\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"DOS:\",dos_data['flag'].value_counts())\n",
    "print()\n",
    "print(\"Impersonation Combined:\",impersonation_data['flag'].value_counts())\n",
    "print()\n",
    "print(\"Smart:\",smart_attack['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dos = dos_data[['can_id', 'dlc', 'data', 'IAT']].values\n",
    "y_dos = dos_data['flag'].values\n",
    "\n",
    "X_imp = impersonation_data[['can_id', 'dlc', 'data', 'IAT']].values\n",
    "y_imp = impersonation_data['flag'].values\n",
    "\n",
    "X_smart = smart_attack.drop(['label'], axis = 1).values\n",
    "y_smart = smart_attack['label']\n",
    "y_tri = y_smart.copy(deep = True)\n",
    "y_smart = y_smart.replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_dos, y_seq_dos = sequencify_data(X_dos, y_dos)\n",
    "X_seq_imp, y_seq_imp = sequencify_data(X_imp, y_imp)\n",
    "X_seq_smart, y_seq_smart = sequencify_data(X_smart, y_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "14746\n"
     ]
    }
   ],
   "source": [
    "print(len(X_seq_dos))\n",
    "print(len(X_seq_imp))\n",
    "print(len(X_seq_smart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([2881, 2119]))\n",
      "(array([0, 1]), array([4186, 5814]))\n",
      "(array([0, 1]), array([ 1350, 13396]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_seq_dos, return_counts=True))\n",
    "print(np.unique(y_seq_imp, return_counts=True))\n",
    "print(np.unique(y_seq_smart, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X_seq, y_seq):\n",
    "    # Get indices for label 0 and label 1\n",
    "    zero_indices = np.where(y_seq == 0)[0]\n",
    "    one_indices = np.where(y_seq == 1)[0]\n",
    "\n",
    "    # Find the number of samples for label 0\n",
    "    num_zeros = len(zero_indices)\n",
    "\n",
    "    # Randomly sample an equal number of samples from label 1\n",
    "    np.random.seed(42)  # Set seed for reproducibility\n",
    "    sampled_one_indices = np.random.choice(one_indices, num_zeros, replace=False)\n",
    "\n",
    "    # Combine the indices of label 0 and sampled label 1\n",
    "    balanced_indices = np.concatenate([zero_indices, sampled_one_indices])\n",
    "\n",
    "    # Shuffle the balanced indices to avoid any ordering issues\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Subset X_seq and y_seq based on the balanced indices\n",
    "    X_seq_balanced = X_seq[balanced_indices]\n",
    "    y_seq_balanced = y_seq[balanced_indices]\n",
    "\n",
    "    return X_seq_balanced, y_seq_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_smart, y_seq_smart = balance_data(X_seq_smart, y_seq_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([1350, 1350]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_seq_smart, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sequencify_data_test(X, y, seq_size=10):\n",
    "#     max_index = len(X) - seq_size + 1\n",
    "\n",
    "#     X_seq = []\n",
    "#     y_seq = []\n",
    "\n",
    "#     for i in range(1000, 10000, seq_size):\n",
    "#         # print(X[i:i+seq_size])  # Append the sequence from DataFrame 'X'\n",
    "#         print(y[i:i+seq_size])\n",
    "#         print(1 if 1 in y[i:i+seq_size] else 0)  # Check for '1' in 'y' values\n",
    "    \n",
    "# sequencify_data_test(X_smart, y_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dos, X_test_dos, y_train_dos, y_test_dos = train_test_split(X_dos, y_dos, test_size=0.3, random_state = 42)\n",
    "X_train_seq_dos, X_test_seq_dos, y_train_seq_dos, y_test_seq_dos = train_test_split(X_seq_dos, y_seq_dos, test_size = 0.3, shuffle = True)\n",
    "\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(X_imp, y_imp, test_size=0.3, random_state = 42)\n",
    "X_train_seq_imp, X_test_seq_imp, y_train_seq_imp, y_test_seq_imp = train_test_split(X_seq_imp, y_seq_imp, test_size = 0.3, shuffle = True)\n",
    "\n",
    "X_train_smart, X_test_smart, y_train_smart, y_test_smart = train_test_split(X_smart, y_smart, test_size=0.3, random_state = 42)\n",
    "X_train_seq_smart, X_test_seq_smart, y_train_seq_smart, y_test_seq_smart = train_test_split(X_seq_smart, y_seq_smart, test_size = 0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_dos = X_train_seq_dos[:2000]\n",
    "X_train_seq_imp = X_train_seq_imp[:2000] \n",
    "X_train_seq_smart = X_train_seq_smart[:2000]\n",
    "\n",
    "y_train_seq_dos = y_train_seq_dos[:2000]\n",
    "y_train_seq_imp = y_train_seq_imp[:2000] \n",
    "y_train_seq_smart = y_train_seq_smart[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(X_train_seq_dos))\n",
    "# print(len(X_train_seq_imp))\n",
    "# print(len(X_train_seq_smart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalizing dataset\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_dos)\n",
    "scaler.fit(X_train_imp)\n",
    "scaler.fit(X_train_smart)\n",
    "\n",
    "X_train = np.concatenate((X_train_dos, X_train_imp, X_train_smart), axis = 0)\n",
    "y_train = np.concatenate((y_train_dos, y_train_imp, y_train_smart), axis = 0)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test_dos = scaler.transform(X_test_dos)\n",
    "X_test_imp = scaler.transform(X_test_imp)\n",
    "X_test_smart = scaler.transform(X_test_smart)\n",
    "\n",
    "mean = np.mean(np.concatenate((X_train_seq_dos, X_train_seq_imp, X_train_seq_smart), axis = 0), axis=(0,1))\n",
    "std = np.mean(np.concatenate((X_train_seq_dos, X_train_seq_imp, X_train_seq_smart), axis = 0), axis=(0,1))\n",
    "\n",
    "X_train_seq = np.concatenate((X_train_seq_dos, X_train_seq_imp, X_train_seq_smart), axis = 0)\n",
    "y_train_seq = np.concatenate((y_train_seq_dos, y_train_seq_imp, y_train_seq_smart), axis = 0)\n",
    "\n",
    "X_train_seq -= mean\n",
    "X_train_seq /= std\n",
    "\n",
    "X_test_seq_dos -= mean\n",
    "X_test_seq_dos /= std\n",
    "\n",
    "X_test_seq_imp -= mean\n",
    "X_test_seq_imp /= std\n",
    "\n",
    "X_test_seq_smart -= mean\n",
    "X_test_seq_smart /= std\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_train_smote, y_train_smote = oversample.fit_resample(X_train, y_train) \n",
    "\n",
    "# seq_class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  classes = np.unique(y_train_seq),\n",
    "#                                                  y = y_train_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([1146,  854]))\n",
      "(array([0, 1]), array([867, 633]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_seq_dos, return_counts=True))\n",
    "print(np.unique(y_test_seq_dos, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 809, 1191]))\n",
      "(array([0, 1]), array([1293, 1707]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_seq_imp, return_counts=True))\n",
    "print(np.unique(y_test_seq_imp, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([943, 947]))\n",
      "(array([0, 1]), array([407, 403]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_seq_smart, return_counts=True))\n",
    "print(np.unique(y_test_seq_smart, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----MLP-------\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 10:02:03.493485: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 10:02:03.498165: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.5420 - accuracy: 0.7393 - val_loss: 0.6664 - val_accuracy: 0.5732\n",
      "Epoch 2/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.4913 - accuracy: 0.7686 - val_loss: 0.7432 - val_accuracy: 0.5793\n",
      "Epoch 3/100\n",
      "7704/7704 [==============================] - 19s 3ms/step - loss: 0.4612 - accuracy: 0.7862 - val_loss: 0.6159 - val_accuracy: 0.6348\n",
      "Epoch 4/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.4409 - accuracy: 0.7951 - val_loss: 0.7224 - val_accuracy: 0.6328\n",
      "Epoch 5/100\n",
      "7704/7704 [==============================] - 21s 3ms/step - loss: 0.4260 - accuracy: 0.8022 - val_loss: 0.5938 - val_accuracy: 0.6925\n",
      "Epoch 6/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.4151 - accuracy: 0.8052 - val_loss: 0.6699 - val_accuracy: 0.6428\n",
      "Epoch 7/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.4086 - accuracy: 0.8087 - val_loss: 0.6296 - val_accuracy: 0.6502\n",
      "Epoch 8/100\n",
      "7704/7704 [==============================] - 21s 3ms/step - loss: 0.4018 - accuracy: 0.8125 - val_loss: 0.5381 - val_accuracy: 0.7051\n",
      "Epoch 9/100\n",
      "7704/7704 [==============================] - 23s 3ms/step - loss: 0.3979 - accuracy: 0.8133 - val_loss: 0.5163 - val_accuracy: 0.7410\n",
      "Epoch 10/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.3940 - accuracy: 0.8155 - val_loss: 0.6577 - val_accuracy: 0.6608\n",
      "Epoch 11/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.3909 - accuracy: 0.8167 - val_loss: 0.4901 - val_accuracy: 0.7459\n",
      "Epoch 12/100\n",
      "7704/7704 [==============================] - 21s 3ms/step - loss: 0.3884 - accuracy: 0.8177 - val_loss: 0.6231 - val_accuracy: 0.6581\n",
      "Epoch 13/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.3852 - accuracy: 0.8194 - val_loss: 0.5349 - val_accuracy: 0.7126\n",
      "Epoch 14/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.3826 - accuracy: 0.8207 - val_loss: 0.5841 - val_accuracy: 0.6819\n",
      "Epoch 15/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.3810 - accuracy: 0.8214 - val_loss: 0.5383 - val_accuracy: 0.6916\n",
      "Epoch 16/100\n",
      "7704/7704 [==============================] - 20s 3ms/step - loss: 0.3786 - accuracy: 0.8224 - val_loss: 0.5774 - val_accuracy: 0.6907\n"
     ]
    }
   ],
   "source": [
    "##MLP\n",
    "\n",
    "print(\"-----MLP-------\")\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Input(shape = (4)))\n",
    "mlp.add(Dense(128, activation = 'relu'))\n",
    "mlp.add(Dense(128, activation = 'relu'))\n",
    "mlp.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "mlp.compile(optimizer='adam',\n",
    "                loss=BinaryCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "mlp_hist = mlp.fit(X_train_smote, y_train_smote, epochs=100, callbacks = [es], validation_split=0.2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----MLP-------\n",
      "2/2 [==============================] - 0s 26ms/step\n",
      "--------DOS--------\n",
      "ACCURACY:  0.8924\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93     11569\n",
      "           1       0.68      0.99      0.81      3431\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.84      0.93      0.87     15000\n",
      "weighted avg       0.92      0.89      0.90     15000\n",
      "\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "--------Impersonation--------\n",
      "ACCURACY:  0.8888\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93     24475\n",
      "           1       0.63      0.97      0.76      5525\n",
      "\n",
      "    accuracy                           0.89     30000\n",
      "   macro avg       0.81      0.92      0.85     30000\n",
      "weighted avg       0.93      0.89      0.90     30000\n",
      "\n",
      "6/6 [==============================] - 0s 11ms/step\n",
      "--------Smart--------\n",
      "ACCURACY:  0.7538823210289564\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.83      0.82     29873\n",
      "         1.0       0.62      0.61      0.62     14366\n",
      "\n",
      "    accuracy                           0.75     44239\n",
      "   macro avg       0.72      0.72      0.72     44239\n",
      "weighted avg       0.75      0.75      0.75     44239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##MLP\n",
    "print(\"-----MLP-------\")\n",
    "threshold = 0.5\n",
    "mlp_preds = mlp.predict(X_test_dos, batch_size = 8196)\n",
    "mlp_preds = (mlp_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------DOS--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_dos, mlp_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_dos, mlp_preds))\n",
    "\n",
    "mlp_preds = mlp.predict(X_test_imp, batch_size = 8196)\n",
    "mlp_preds = (mlp_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------Impersonation--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_imp, mlp_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_imp, mlp_preds))\n",
    "\n",
    "\n",
    "mlp_preds = mlp.predict(X_test_smart, batch_size = 8196)\n",
    "mlp_preds = (mlp_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------Smart--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_smart, mlp_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_smart, mlp_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_class_weights = dict(enumerate(seq_class_weights))\n",
    "# print(seq_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dict(enumerate(class_weight.compute_class_weight('balanced',\n",
    "#                                   classes = np.unique(y_train_seq_smart),\n",
    "#                                 y = y_train_seq_smart))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM-------\n",
      "Epoch 1/1000\n",
      "148/148 [==============================] - 13s 79ms/step - loss: 0.5256 - accuracy: 0.7207 - val_loss: 0.6443 - val_accuracy: 0.6070\n",
      "Epoch 2/1000\n",
      "148/148 [==============================] - 3s 23ms/step - loss: 0.3842 - accuracy: 0.8262 - val_loss: 0.5880 - val_accuracy: 0.6902\n",
      "Epoch 3/1000\n",
      "148/148 [==============================] - 5s 37ms/step - loss: 0.3382 - accuracy: 0.8589 - val_loss: 0.5955 - val_accuracy: 0.7071\n",
      "Epoch 4/1000\n",
      "148/148 [==============================] - 6s 41ms/step - loss: 0.2648 - accuracy: 0.8896 - val_loss: 0.5943 - val_accuracy: 0.7088\n",
      "Epoch 5/1000\n",
      "148/148 [==============================] - 9s 60ms/step - loss: 0.2196 - accuracy: 0.9096 - val_loss: 0.6489 - val_accuracy: 0.7165\n",
      "Epoch 6/1000\n",
      "148/148 [==============================] - 11s 76ms/step - loss: 0.1997 - accuracy: 0.9162 - val_loss: 0.5567 - val_accuracy: 0.7555\n",
      "Epoch 7/1000\n",
      "148/148 [==============================] - 6s 39ms/step - loss: 0.1717 - accuracy: 0.9298 - val_loss: 0.5776 - val_accuracy: 0.7513\n",
      "Epoch 8/1000\n",
      "148/148 [==============================] - 8s 56ms/step - loss: 0.1866 - accuracy: 0.9234 - val_loss: 0.4631 - val_accuracy: 0.7878\n",
      "Epoch 9/1000\n",
      "148/148 [==============================] - 7s 46ms/step - loss: 0.1733 - accuracy: 0.9291 - val_loss: 0.5248 - val_accuracy: 0.7657\n",
      "Epoch 10/1000\n",
      "148/148 [==============================] - 5s 37ms/step - loss: 0.1504 - accuracy: 0.9399 - val_loss: 0.5368 - val_accuracy: 0.7725\n",
      "Epoch 11/1000\n",
      "148/148 [==============================] - 11s 71ms/step - loss: 0.1387 - accuracy: 0.9450 - val_loss: 0.4411 - val_accuracy: 0.8183\n",
      "Epoch 12/1000\n",
      "148/148 [==============================] - 3s 20ms/step - loss: 0.1372 - accuracy: 0.9444 - val_loss: 0.4155 - val_accuracy: 0.8048\n",
      "Epoch 13/1000\n",
      "148/148 [==============================] - 6s 40ms/step - loss: 0.1245 - accuracy: 0.9491 - val_loss: 0.4737 - val_accuracy: 0.7971\n",
      "Epoch 14/1000\n",
      "148/148 [==============================] - 4s 26ms/step - loss: 0.1372 - accuracy: 0.9419 - val_loss: 0.4875 - val_accuracy: 0.8005\n",
      "Epoch 15/1000\n",
      "148/148 [==============================] - 11s 76ms/step - loss: 0.1109 - accuracy: 0.9556 - val_loss: 0.4536 - val_accuracy: 0.8065\n",
      "Epoch 16/1000\n",
      "148/148 [==============================] - 8s 52ms/step - loss: 0.1113 - accuracy: 0.9554 - val_loss: 0.5905 - val_accuracy: 0.7750\n",
      "Epoch 17/1000\n",
      "148/148 [==============================] - 7s 48ms/step - loss: 0.1201 - accuracy: 0.9489 - val_loss: 0.4395 - val_accuracy: 0.8115\n"
     ]
    }
   ],
   "source": [
    "##LSTM\n",
    "\n",
    "print(\"-----LSTM-------\")\n",
    "\n",
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(Input(shape = X_train_seq.shape[1:]))\n",
    "lstm.add(LSTM(128, activation = 'relu'))\n",
    "lstm.add(Dense(64, activation = 'relu'))\n",
    "lstm.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "lstm.compile(\n",
    "    loss = BinaryCrossentropy(from_logits = False),\n",
    "    optimizer = Adam(learning_rate = 0.001),\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "lstm_hist = lstm.fit(X_train_seq, y_train_seq, batch_size = 32, validation_split = 0.2,\n",
    "        callbacks = [es], epochs = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM-------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 241ms/step\n",
      "--------DOS--------\n",
      "ACCURACY:  0.97\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       867\n",
      "           1       0.93      1.00      0.97       633\n",
      "\n",
      "    accuracy                           0.97      1500\n",
      "   macro avg       0.97      0.97      0.97      1500\n",
      "weighted avg       0.97      0.97      0.97      1500\n",
      "\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "--------Impersonation--------\n",
      "ACCURACY:  0.9463333333333334\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      1293\n",
      "           1       0.94      0.97      0.95      1707\n",
      "\n",
      "    accuracy                           0.95      3000\n",
      "   macro avg       0.95      0.94      0.94      3000\n",
      "weighted avg       0.95      0.95      0.95      3000\n",
      "\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "--------Smart--------\n",
      "ACCURACY:  0.8345679012345679\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.77      0.82       407\n",
      "           1       0.80      0.90      0.84       403\n",
      "\n",
      "    accuracy                           0.83       810\n",
      "   macro avg       0.84      0.83      0.83       810\n",
      "weighted avg       0.84      0.83      0.83       810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-----LSTM-------\")\n",
    "\n",
    "lstm_preds = lstm.predict(X_test_seq_dos, batch_size=4096)\n",
    "lstm_preds = (lstm_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------DOS--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_seq_dos, lstm_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_seq_dos, lstm_preds))\n",
    "\n",
    "\n",
    "lstm_preds = lstm.predict(X_test_seq_imp, batch_size=4096)\n",
    "lstm_preds = (lstm_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------Impersonation--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_seq_imp, lstm_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_seq_imp, lstm_preds))\n",
    "\n",
    "\n",
    "lstm_preds = lstm.predict(X_test_seq_smart, batch_size=4096)\n",
    "lstm_preds = (lstm_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------Smart--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_seq_smart, lstm_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_seq_smart, lstm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------DECISION TREE--------\n",
      "--------DOS--------\n",
      "ACCURACY:  0.6616\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.56      0.72     11569\n",
      "           1       0.40      1.00      0.57      3431\n",
      "\n",
      "    accuracy                           0.66     15000\n",
      "   macro avg       0.70      0.78      0.65     15000\n",
      "weighted avg       0.86      0.66      0.69     15000\n",
      "\n",
      "--------Impersonation--------\n",
      "ACCURACY:  0.6415666666666666\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.57      0.72     24475\n",
      "           1       0.34      0.97      0.50      5525\n",
      "\n",
      "    accuracy                           0.64     30000\n",
      "   macro avg       0.66      0.77      0.61     30000\n",
      "weighted avg       0.87      0.64      0.68     30000\n",
      "\n",
      "--------Smart--------\n",
      "ACCURACY:  0.6186396618368408\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.58      0.67     29873\n",
      "         1.0       0.44      0.70      0.55     14366\n",
      "\n",
      "    accuracy                           0.62     44239\n",
      "   macro avg       0.62      0.64      0.61     44239\n",
      "weighted avg       0.69      0.62      0.63     44239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth = 4)\n",
    "dt.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "dt_preds = dt.predict(X_test_dos)\n",
    "\n",
    "print(\"-------DECISION TREE--------\")\n",
    "\n",
    "print(\"--------DOS--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_dos, dt_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_dos, dt_preds))\n",
    "    \n",
    "\n",
    "dt_preds = dt.predict(X_test_imp)\n",
    "\n",
    "print(\"--------Impersonation--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_imp, dt_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_imp, dt_preds))\n",
    "\n",
    "dt_preds = dt.predict(X_test_smart)\n",
    "\n",
    "print(\"--------Smart--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_smart, dt_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_smart, dt_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
