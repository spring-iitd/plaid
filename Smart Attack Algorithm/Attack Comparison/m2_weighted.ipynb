{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 09:49:17.281833: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from copy import deepcopy\n",
    "from sklearn.utils import class_weight\n",
    "import joblib\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_attack_path = 'attack_10_10.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_bin(hex_num):\n",
    "    \n",
    "    binary_value = bin(int(str(hex_num), 16))[2:]\n",
    "    \n",
    "    return binary_value\n",
    "\n",
    "def int_to_bin(int_num):\n",
    "    \n",
    "    binary_value = bin(int_num)[2:]\n",
    "    \n",
    "    return binary_value\n",
    "\n",
    "def pad(value, length):\n",
    "    \n",
    "    curr_length = len(str(value))\n",
    "    \n",
    "    zeros = '0' * (length - curr_length)\n",
    "    \n",
    "    return zeros + value\n",
    "\n",
    "hex_to_dec = lambda x: int(x, 16)\n",
    "\n",
    "def transform_data(data):\n",
    "\n",
    "    data['ID'] = data['ID'].apply(hex_to_dec)\n",
    "    data['Payload'] = data['Payload'].apply(hex_to_dec)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_columns(df):\n",
    "    \n",
    "    for dlc in [2,5,6]:\n",
    "\n",
    "        df.loc[df['dlc'] == dlc, df.columns[3:]] = df.loc[df['dlc'] == dlc, df.columns[3:]].shift(periods=8-dlc, axis='columns', fill_value='00')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_attack_data(data_path):\n",
    "    \n",
    "    columns = ['timestamp','can_id', 'dlc', 'data0', 'data1', 'data2', 'data3', 'data4', \n",
    "           'data5', 'data6', 'data7', 'flag']\n",
    "    \n",
    "    data = pd.read_csv(data_path, names = columns)\n",
    "\n",
    "    data = shift_columns(data)\n",
    "    \n",
    "    ##Replacing all NaNs with '00' \n",
    "    data = data.replace(np.NaN, '00')\n",
    "    \n",
    "    ##Joining all data columns to put all data in one column\n",
    "    data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "    ##The data column is in hexadecimal\n",
    "    data['data'] = data[data_cols].apply(''.join, axis=1)\n",
    "    data.drop(columns = data_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    ##Converting columns to decimal\n",
    "    data['can_id'] = data['can_id'].apply(hex_to_dec)\n",
    "    data['data'] = data['data'].apply(hex_to_dec)\n",
    "\n",
    "    data = data.assign(IAT=data['timestamp'].diff().fillna(0))\n",
    "    \n",
    "    return data[:150_000]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_attack = pd.read_csv(smart_attack_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequencify_data(X, y, seq_size=10):\n",
    "    max_index = len(X) - seq_size + 1\n",
    "\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for i in range(0, max_index, seq_size):\n",
    "        X_seq.append(X[i:i+seq_size])  # Append the sequence from DataFrame 'X'\n",
    "        try:\n",
    "            y_seq.append(1 if 1 in y[i:i+seq_size].values else 0)  # Check for '1' in 'y' values\n",
    "        except:\n",
    "             y_seq.append(1 if 1 in y[i:i+seq_size] else 0)\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_attack.drop(columns = ['Timestamp'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smart = smart_attack.drop(['label'], axis = 1).values\n",
    "y_smart = smart_attack['label']\n",
    "y_tri = y_smart.copy(deep = True)\n",
    "y_smart = y_smart.replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([100000,  47462]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_smart, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_seq_smart, y_seq_smart = sequencify_data(X_smart, y_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 1350, 13396]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_seq_smart, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(X_seq, y_seq):\n",
    "    # Get indices for label 0 and label 1\n",
    "    zero_indices = np.where(y_seq == 0)[0]\n",
    "    one_indices = np.where(y_seq == 1)[0]\n",
    "\n",
    "    # Find the number of samples for label 0\n",
    "    num_zeros = len(zero_indices)\n",
    "\n",
    "    # Randomly sample an equal number of samples from label 1\n",
    "    np.random.seed(42)  # Set seed for reproducibility\n",
    "    sampled_one_indices = np.random.choice(one_indices, num_zeros, replace=False)\n",
    "\n",
    "    # Combine the indices of label 0 and sampled label 1\n",
    "    balanced_indices = np.concatenate([zero_indices, sampled_one_indices])\n",
    "\n",
    "    # Shuffle the balanced indices to avoid any ordering issues\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Subset X_seq and y_seq based on the balanced indices\n",
    "    X_seq_balanced = X_seq[balanced_indices]\n",
    "    y_seq_balanced = y_seq[balanced_indices]\n",
    "\n",
    "    return X_seq_balanced, y_seq_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_seq_smart, y_seq_smart = balance_data(X_seq_smart, y_seq_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1]), array([ 1350, 13396]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_seq_smart, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smart, X_test_smart, y_train_smart, y_test_smart = train_test_split(X_smart, y_smart, test_size=0.3, random_state = 42)\n",
    "X_train_seq_smart, X_test_seq_smart, y_train_seq_smart, y_test_seq_smart = train_test_split(X_seq_smart, y_seq_smart, test_size = 0.3, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10322, 10, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_smart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = joblib.load('scaler_m0.sav')\n",
    "\n",
    "X_train_smart = scaler.fit_transform(X_train_smart)\n",
    "X_test_smart = scaler.transform(X_test_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = np.mean(X_train_seq_smart, axis=(0, 1))  # Mean of each feature across training samples and timesteps\n",
    "train_stds = np.std(X_train_seq_smart, axis=(0, 1))    # Standard deviation of each feature across training samples and timesteps\n",
    "\n",
    "# Handle case where std is zero (to avoid division by zero)\n",
    "train_stds[train_stds == 0] = 1e-8\n",
    "\n",
    "# Standardize the training set\n",
    "X_train_seq_smart = (X_train_seq_smart - train_means) / train_stds\n",
    "\n",
    "# Standardize the test set using the training set's mean and std\n",
    "X_test_seq_smart = (X_test_seq_smart - train_means) / train_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10322, 10, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_seq_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4424, 10, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_test_seq_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103223, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train_smart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = class_weight.compute_class_weight('balanced',\n",
    "#                                                  classes = np.unique(y_train_smart),\n",
    "#                                                  y = y_train_smart)\n",
    "# class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "\n",
    "seq_class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 classes = np.unique(y_train_seq_smart),\n",
    "                                                 y = y_train_seq_smart)\n",
    "seq_class_weights = dict(enumerate(seq_class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5.508004268943436, 1: 0.5499200852424081}\n"
     ]
    }
   ],
   "source": [
    "print(seq_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "X_train_smote, y_train_smote = oversample.fit_resample(X_train_smart, y_train_smart) \n",
    "# X_train_smote, y_train_smote = X_train, y_train_smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1.]), array([70127, 70127]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train_smote, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----MLP-------\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 09:49:19.395060: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-15 09:49:19.399757: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3507/3507 [==============================] - 10s 3ms/step - loss: 0.9124 - accuracy: 0.6833 - val_loss: 0.7282 - val_accuracy: 0.4593\n",
      "Epoch 2/100\n",
      "3507/3507 [==============================] - 10s 3ms/step - loss: 0.5923 - accuracy: 0.6958 - val_loss: 0.8814 - val_accuracy: 0.3017\n",
      "Epoch 3/100\n",
      "3507/3507 [==============================] - 10s 3ms/step - loss: 0.5784 - accuracy: 0.7011 - val_loss: 1.0503 - val_accuracy: 0.3544\n",
      "Epoch 4/100\n",
      "3507/3507 [==============================] - 8s 2ms/step - loss: 0.5695 - accuracy: 0.7071 - val_loss: 0.5760 - val_accuracy: 0.6224\n",
      "Epoch 5/100\n",
      "3507/3507 [==============================] - 9s 3ms/step - loss: 0.5628 - accuracy: 0.7134 - val_loss: 0.7227 - val_accuracy: 0.4535\n",
      "Epoch 6/100\n",
      "3507/3507 [==============================] - 9s 3ms/step - loss: 0.5530 - accuracy: 0.7248 - val_loss: 0.7025 - val_accuracy: 0.4690\n",
      "Epoch 7/100\n",
      "3507/3507 [==============================] - 9s 3ms/step - loss: 0.5424 - accuracy: 0.7355 - val_loss: 0.8338 - val_accuracy: 0.4735\n",
      "Epoch 8/100\n",
      "3507/3507 [==============================] - 9s 3ms/step - loss: 0.5354 - accuracy: 0.7422 - val_loss: 0.7918 - val_accuracy: 0.4499\n",
      "Epoch 9/100\n",
      "3507/3507 [==============================] - 8s 2ms/step - loss: 0.5211 - accuracy: 0.7508 - val_loss: 0.6869 - val_accuracy: 0.5320\n"
     ]
    }
   ],
   "source": [
    "##MLP\n",
    "\n",
    "print(\"-----MLP-------\")\n",
    "\n",
    "mlp = load_model('mlp_m0.h5')\n",
    "\n",
    "mlp.compile(optimizer='adam',\n",
    "                loss=BinaryCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "mlp_hist = mlp.fit(X_train_smote, y_train_smote, epochs=100, callbacks = [es], validation_split=0.2, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----MLP-------\n",
      " 195/1383 [===>..........................] - ETA: 2s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383/1383 [==============================] - 2s 1ms/step\n",
      "--------Smart--------\n",
      "ACCURACY:  0.6421935396369719\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.65      0.71     29873\n",
      "         1.0       0.46      0.62      0.53     14366\n",
      "\n",
      "    accuracy                           0.64     44239\n",
      "   macro avg       0.62      0.64      0.62     44239\n",
      "weighted avg       0.68      0.64      0.65     44239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##MLP\n",
    "print(\"-----MLP-------\")\n",
    "threshold = 0.5\n",
    "mlp_preds = mlp.predict(X_test_smart, batch_size = 32)\n",
    "mlp_preds = (mlp_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------Smart--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_smart, mlp_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_smart, mlp_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM-------\n",
      "Epoch 1/1000\n",
      "259/259 [==============================] - 6s 19ms/step - loss: 0.7671 - accuracy: 0.6468 - val_loss: 0.5202 - val_accuracy: 0.7114\n",
      "Epoch 2/1000\n",
      "259/259 [==============================] - 7s 28ms/step - loss: 0.4942 - accuracy: 0.7701 - val_loss: 0.5146 - val_accuracy: 0.7787\n",
      "Epoch 3/1000\n",
      "259/259 [==============================] - 12s 44ms/step - loss: 0.3647 - accuracy: 0.8353 - val_loss: 0.4467 - val_accuracy: 0.7879\n",
      "Epoch 4/1000\n",
      "259/259 [==============================] - 9s 35ms/step - loss: 0.2554 - accuracy: 0.9031 - val_loss: 0.2864 - val_accuracy: 0.8925\n",
      "Epoch 5/1000\n",
      "259/259 [==============================] - 6s 22ms/step - loss: 0.2255 - accuracy: 0.9134 - val_loss: 0.1728 - val_accuracy: 0.9400\n",
      "Epoch 6/1000\n",
      "259/259 [==============================] - 7s 25ms/step - loss: 0.1826 - accuracy: 0.9310 - val_loss: 0.1653 - val_accuracy: 0.9346\n",
      "Epoch 7/1000\n",
      "259/259 [==============================] - 11s 42ms/step - loss: 0.2081 - accuracy: 0.9262 - val_loss: 0.1675 - val_accuracy: 0.9298\n",
      "Epoch 8/1000\n",
      "259/259 [==============================] - 10s 37ms/step - loss: 0.1664 - accuracy: 0.9368 - val_loss: 0.1258 - val_accuracy: 0.9540\n",
      "Epoch 9/1000\n",
      "259/259 [==============================] - 25s 96ms/step - loss: 0.1381 - accuracy: 0.9449 - val_loss: 0.1106 - val_accuracy: 0.9574\n",
      "Epoch 10/1000\n",
      "259/259 [==============================] - 33s 126ms/step - loss: 0.1302 - accuracy: 0.9476 - val_loss: 0.1668 - val_accuracy: 0.9269\n",
      "Epoch 11/1000\n",
      "259/259 [==============================] - 11s 44ms/step - loss: 0.1060 - accuracy: 0.9560 - val_loss: 0.1423 - val_accuracy: 0.9448\n",
      "Epoch 12/1000\n",
      "259/259 [==============================] - 5s 19ms/step - loss: 0.1291 - accuracy: 0.9483 - val_loss: 0.1205 - val_accuracy: 0.9496\n",
      "Epoch 13/1000\n",
      "259/259 [==============================] - 8s 29ms/step - loss: 0.1151 - accuracy: 0.9556 - val_loss: 0.1933 - val_accuracy: 0.9240\n",
      "Epoch 14/1000\n",
      "259/259 [==============================] - 11s 44ms/step - loss: 0.1856 - accuracy: 0.9201 - val_loss: 0.1457 - val_accuracy: 0.9414\n"
     ]
    }
   ],
   "source": [
    "##LSTM\n",
    "\n",
    "print(\"-----LSTM-------\")\n",
    "\n",
    "lstm = load_model('lstm_m0_weighted.h5')\n",
    "\n",
    "lstm.compile(\n",
    "    loss = BinaryCrossentropy(from_logits = False),\n",
    "    optimizer = Adam(learning_rate = 0.001),\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "lstm_hist = lstm.fit(X_train_seq_smart, y_train_seq_smart, batch_size = 32, validation_split = 0.2,\n",
    "        callbacks = [es], epochs = 1000, class_weight=seq_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM-------\n",
      "139/139 [==============================] - 1s 3ms/step\n",
      "--------Smart--------\n",
      "ACCURACY:  0.9579566003616636\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.85      0.79       413\n",
      "           1       0.98      0.97      0.98      4011\n",
      "\n",
      "    accuracy                           0.96      4424\n",
      "   macro avg       0.86      0.91      0.88      4424\n",
      "weighted avg       0.96      0.96      0.96      4424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-----LSTM-------\")\n",
    "\n",
    "lstm_preds = lstm.predict(X_test_seq_smart, batch_size=32)\n",
    "lstm_preds = (lstm_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"--------Smart--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_seq_smart, lstm_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_seq_smart, lstm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Smart--------\n",
      "ACCURACY:  0.7346685051651258\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.87      0.82     29873\n",
      "         1.0       0.63      0.45      0.52     14366\n",
      "\n",
      "    accuracy                           0.73     44239\n",
      "   macro avg       0.70      0.66      0.67     44239\n",
      "weighted avg       0.72      0.73      0.72     44239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = joblib.load('dt_m0.pkl')\n",
    "dt.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "dt_preds = dt.predict(X_test_smart)\n",
    "\n",
    "print(\"--------Smart--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test_smart, dt_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test_smart, dt_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
