{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39bb9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, save_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import argparse\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# parser.add_argument('directory', \n",
    "#                     type=str, \n",
    "#                     help = 'directory where files are stored')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "base_dir = '../data/Car Hacking Dataset/'\n",
    "\n",
    "\n",
    "file_name = 'smart_output.csv'\n",
    "data = pd.read_csv(os.path.join(base_dir, file_name))\n",
    "\n",
    "# save_loc = os.path.join(base_dir, 'Adversarial Training Evaluation')\n",
    "# os.makedirs(save_loc)\n",
    "\n",
    "data = data.assign(IAT=data['Timestamp'].diff().fillna(0))\n",
    "data.drop(['Timestamp'], axis = 1, inplace = True)\n",
    "\n",
    "def hex_to_bin(hex_num):\n",
    "    \n",
    "    binary_value = bin(int(str(hex_num), 16))[2:]\n",
    "    \n",
    "    return binary_value\n",
    "\n",
    "def int_to_bin(int_num):\n",
    "    \n",
    "    binary_value = bin(int_num)[2:]\n",
    "    \n",
    "    return binary_value\n",
    "\n",
    "def pad(value, length):\n",
    "    \n",
    "    curr_length = len(str(value))\n",
    "    \n",
    "    zeros = '0' * (length - curr_length)\n",
    "    \n",
    "    return zeros + value\n",
    "\n",
    "hex_to_dec = lambda x: int(x, 16)\n",
    "\n",
    "def transform_data(data):\n",
    "\n",
    "    data['ID'] = data['ID'].apply(hex_to_dec)\n",
    "    data['Payload'] = data['Payload'].apply(hex_to_dec)\n",
    "\n",
    "    return data\n",
    "\n",
    "def sequencify_data(X, y, seq_size=10):\n",
    "    max_index = len(X) - seq_size + 1\n",
    "\n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "\n",
    "    for i in range(0, max_index, seq_size):\n",
    "        X_seq.append(X[i:i+seq_size])  # Append the sequence from DataFrame 'X'\n",
    "        y_seq.append(1 if 1 in y[i:i+seq_size] else 0)  # Check for '1' in 'y' values\n",
    "\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# data = transform_data(data)\n",
    "\n",
    "X = data.drop('label', axis = 1)\n",
    "y = data['label']\n",
    "\n",
    "X_seq, y_seq = sequencify_data(X.values, y.values)\n",
    "\n",
    "#Splitting into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle=True)\n",
    "X_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(X_seq, y_seq, test_size = 0.2, shuffle= True)\n",
    "\n",
    "#Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "seq_scaler = StandardScaler()\n",
    "num_train_samples, seq_length, num_features = X_seq_train.shape\n",
    "num_test_samples, _, _ = X_seq_test.shape\n",
    "\n",
    "X_train_seq_reshaped = X_seq_train.reshape(num_train_samples, -1)\n",
    "X_test_seq_reshaped = X_seq_test.reshape(num_test_samples, -1)\n",
    "\n",
    "X_train_seq_scaled = seq_scaler.fit_transform(X_train_seq_reshaped)\n",
    "X_test_seq_scaled = seq_scaler.transform(X_test_seq_reshaped)\n",
    "\n",
    "# Reshape the scaled data back to the original shape\n",
    "X_seq_train = X_train_seq_scaled.reshape(num_train_samples, seq_length, num_features)\n",
    "X_seq_test = X_test_seq_scaled.reshape(num_test_samples, seq_length, num_features)\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_train_smote, y_train_smote = oversample.fit_resample(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d99ff631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), array([93783,  8220]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_seq, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0b23015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 1.]), array([791201, 791201]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_smote, return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbde6029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----MLP-------\n",
      "Epoch 1/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 0.0281 - accuracy: 0.9917 - val_loss: 8.1518e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 5.5750e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 8.3597e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 0.0014 - accuracy: 0.9998 - val_loss: 8.0769e-05 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 7.2796e-05 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.6575e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 3.8575e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 9.8846e-04 - accuracy: 0.9998 - val_loss: 8.1613e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 9.0616e-04 - accuracy: 0.9999 - val_loss: 3.6267e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 8.4429e-04 - accuracy: 0.9999 - val_loss: 4.3434e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 7.5886e-04 - accuracy: 0.9999 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 8.2721e-04 - accuracy: 0.9999 - val_loss: 4.7597e-05 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "9891/9891 [==============================] - 15s 2ms/step - loss: 6.9887e-04 - accuracy: 0.9999 - val_loss: 4.0329e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "9891/9891 [==============================] - 16s 2ms/step - loss: 6.7710e-04 - accuracy: 0.9999 - val_loss: 7.1057e-05 - val_accuracy: 1.0000\n",
      "-----MLP-------\n",
      "ACCURACY:  0.9997843221834438\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    197670\n",
      "         1.0       0.99      1.00      1.00      6338\n",
      "\n",
      "    accuracy                           1.00    204008\n",
      "   macro avg       1.00      1.00      1.00    204008\n",
      "weighted avg       1.00      1.00      1.00    204008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Models\n",
    "\n",
    "print(\"-----MLP-------\")\n",
    "\n",
    "mlp = Sequential()\n",
    "mlp.add(Input(shape = (4)))\n",
    "mlp.add(Dense(128, activation = 'relu'))\n",
    "mlp.add(Dense(128, activation = 'relu'))\n",
    "mlp.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "mlp.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "mlp_hist = mlp.fit(X_train_smote, y_train_smote, epochs=100, callbacks = [es], validation_split=0.2, batch_size = 128)\n",
    "\n",
    "##MLP\n",
    "print(\"-----MLP-------\")\n",
    "\n",
    "threshold = 0.5\n",
    "mlp_preds = mlp.predict(X_test)\n",
    "mlp_preds = (mlp_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"ACCURACY: \", accuracy_score(y_test, mlp_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test, mlp_preds))\n",
    "\n",
    "with open(os.path.join(save_loc,'train_and_eval_results.txt'),'w') as file:\n",
    "    file.write(\"-------MLP-------\\n\")\n",
    "    file.write(f\"Accuracy Score: \")\n",
    "    file.write(str(accuracy_score(y_test, mlp_preds)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write('Classification Report:\\n')\n",
    "    file.write(str(classification_report(y_test, mlp_preds)))\n",
    "    file.write(\"\\n\\n\\n\\n\")\n",
    "\n",
    "mlp.save(os.path.join(save_loc, 'mlp.h5'))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(mlp_hist.history['loss'])\n",
    "plt.plot(mlp_hist.history['val_loss'])\n",
    "plt.title('MLP Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(save_loc,'mlp_training_history.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6395c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----LSTM-------\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/1000\n",
      "511/511 [==============================] - 7s 11ms/step - loss: 0.1948 - accuracy: 0.9393 - val_loss: 0.0707 - val_accuracy: 0.9763\n",
      "Epoch 2/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0522 - accuracy: 0.9815 - val_loss: 0.0332 - val_accuracy: 0.9876\n",
      "Epoch 3/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0359 - accuracy: 0.9866 - val_loss: 0.0276 - val_accuracy: 0.9895\n",
      "Epoch 4/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0292 - accuracy: 0.9892 - val_loss: 0.0221 - val_accuracy: 0.9914\n",
      "Epoch 5/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0243 - accuracy: 0.9904 - val_loss: 0.0211 - val_accuracy: 0.9908\n",
      "Epoch 6/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0196 - accuracy: 0.9918 - val_loss: 0.0225 - val_accuracy: 0.9922\n",
      "Epoch 7/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0194 - accuracy: 0.9922 - val_loss: 0.0199 - val_accuracy: 0.9920\n",
      "Epoch 8/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0161 - accuracy: 0.9927 - val_loss: 0.0191 - val_accuracy: 0.9923\n",
      "Epoch 9/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0156 - accuracy: 0.9937 - val_loss: 0.0242 - val_accuracy: 0.9902\n",
      "Epoch 10/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0139 - accuracy: 0.9944 - val_loss: 0.0149 - val_accuracy: 0.9945\n",
      "Epoch 11/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0131 - accuracy: 0.9944 - val_loss: 0.0111 - val_accuracy: 0.9955\n",
      "Epoch 12/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0102 - accuracy: 0.9956 - val_loss: 0.0131 - val_accuracy: 0.9956\n",
      "Epoch 13/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0099 - accuracy: 0.9960 - val_loss: 0.0126 - val_accuracy: 0.9957\n",
      "Epoch 14/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0091 - accuracy: 0.9965 - val_loss: 0.0114 - val_accuracy: 0.9960\n",
      "Epoch 15/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 0.0126 - val_accuracy: 0.9953\n",
      "Epoch 16/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0076 - accuracy: 0.9967 - val_loss: 0.0097 - val_accuracy: 0.9963\n",
      "Epoch 17/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0080 - accuracy: 0.9968 - val_loss: 0.0105 - val_accuracy: 0.9966\n",
      "Epoch 18/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0071 - accuracy: 0.9974 - val_loss: 0.0079 - val_accuracy: 0.9969\n",
      "Epoch 19/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0064 - accuracy: 0.9973 - val_loss: 0.0076 - val_accuracy: 0.9975\n",
      "Epoch 20/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0053 - accuracy: 0.9980 - val_loss: 0.0073 - val_accuracy: 0.9974\n",
      "Epoch 21/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0058 - accuracy: 0.9975 - val_loss: 0.0075 - val_accuracy: 0.9975\n",
      "Epoch 22/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0048 - accuracy: 0.9981 - val_loss: 0.0068 - val_accuracy: 0.9977\n",
      "Epoch 23/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0068 - val_accuracy: 0.9979\n",
      "Epoch 24/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.0116 - val_accuracy: 0.9970\n",
      "Epoch 25/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0072 - val_accuracy: 0.9979\n",
      "Epoch 26/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0057 - val_accuracy: 0.9982\n",
      "Epoch 27/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0061 - val_accuracy: 0.9977\n",
      "Epoch 28/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0065 - val_accuracy: 0.9983\n",
      "Epoch 29/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0067 - val_accuracy: 0.9979\n",
      "Epoch 30/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0062 - val_accuracy: 0.9985\n",
      "Epoch 31/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0055 - val_accuracy: 0.9982\n",
      "Epoch 32/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0072 - val_accuracy: 0.9982\n",
      "Epoch 33/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0066 - val_accuracy: 0.9985\n",
      "Epoch 34/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.0055 - val_accuracy: 0.9987\n",
      "Epoch 35/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0054 - val_accuracy: 0.9985\n",
      "Epoch 36/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0075 - val_accuracy: 0.9980\n",
      "Epoch 37/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0050 - val_accuracy: 0.9991\n",
      "Epoch 38/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.0060 - val_accuracy: 0.9987\n",
      "Epoch 39/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0052 - val_accuracy: 0.9981\n",
      "Epoch 40/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0046 - val_accuracy: 0.9987\n",
      "Epoch 41/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
      "Epoch 42/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0051 - val_accuracy: 0.9985\n",
      "Epoch 43/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 9.6486e-04 - accuracy: 0.9999 - val_loss: 0.0083 - val_accuracy: 0.9982\n",
      "Epoch 44/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0059 - val_accuracy: 0.9985\n",
      "Epoch 45/1000\n",
      "511/511 [==============================] - 5s 10ms/step - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0064 - val_accuracy: 0.9980\n",
      "-----LSTM-------\n",
      "ACCURACY:  0.9991667075143376\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     18771\n",
      "           1       0.99      1.00      0.99      1630\n",
      "\n",
      "    accuracy                           1.00     20401\n",
      "   macro avg       1.00      1.00      1.00     20401\n",
      "weighted avg       1.00      1.00      1.00     20401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##LSTM\n",
    "\n",
    "print(\"-----LSTM-------\")\n",
    "\n",
    "lstm = Sequential()\n",
    "\n",
    "lstm.add(Input(shape = X_seq_train.shape[1:]))\n",
    "lstm.add(LSTM(128, activation = 'relu'))\n",
    "lstm.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "lstm.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "lstm_hist = lstm.fit(X_seq_train, y_seq_train, batch_size = 128, validation_split = 0.2,\n",
    "        callbacks = [es], epochs = 1000)\n",
    "\n",
    "print(\"-----LSTM-------\")\n",
    "\n",
    "lstm_preds = lstm.predict(X_seq_test, batch_size=4096)\n",
    "lstm_preds = (lstm_preds >= threshold).astype(int)\n",
    "\n",
    "print(\"ACCURACY: \", accuracy_score(y_seq_test, lstm_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_seq_test, lstm_preds))\n",
    "\n",
    "with open(os.path.join(save_loc,'train_and_eval_results.txt'),'a') as file:\n",
    "    file.write(\"-------LSTM-------\\n\")\n",
    "    file.write(f\"Accuracy Score: \")\n",
    "    file.write(str(accuracy_score(y_seq_test, lstm_preds)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write('Classification Report:\\n')\n",
    "    file.write(str(classification_report(y_seq_test, lstm_preds)))\n",
    "    file.write(\"\\n\\n\\n\\n\")\n",
    "\n",
    "lstm.save(os.path.join(save_loc, 'lstm.h5'))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(lstm_hist.history['loss'])\n",
    "plt.plot(lstm_hist.history['val_loss'])\n",
    "plt.title('LSTM Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.savefig(os.path.join(save_loc,'lstm_training_history.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e25ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/visitor/anwesh.visitor/.conda/envs/tf/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:14:40] WARNING: /croot/xgboost-split_1675119646044/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-------XGBOOST-------\n",
      "ACCURACY:  0.999995098231442\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    197670\n",
      "         1.0       1.00      1.00      1.00      6338\n",
      "\n",
      "    accuracy                           1.00    204008\n",
      "   macro avg       1.00      1.00      1.00    204008\n",
      "weighted avg       1.00      1.00      1.00    204008\n",
      "\n",
      "-------DECISION TREE--------\n",
      "ACCURACY:  0.9648690247441277\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98    197670\n",
      "         1.0       0.47      1.00      0.64      6338\n",
      "\n",
      "    accuracy                           0.96    204008\n",
      "   macro avg       0.73      0.98      0.81    204008\n",
      "weighted avg       0.98      0.96      0.97    204008\n",
      "\n",
      "-------RANDOM FOREST-------\n",
      "\n",
      "ACCURACY:  0.999593153209678\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00    197670\n",
      "         1.0       0.99      1.00      0.99      6338\n",
      "\n",
      "    accuracy                           1.00    204008\n",
      "   macro avg       0.99      1.00      1.00    204008\n",
      "weighted avg       1.00      1.00      1.00    204008\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## XGBOOST\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_smote, y_train_smote)\n",
    "xgb_preds = xgb.predict(X_test)\n",
    "\n",
    "print(\"-------XGBOOST-------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test, xgb_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test, xgb_preds))\n",
    "xgb.save_model(os.path.join(save_loc, 'xgb.json'))\n",
    "\n",
    "with open(os.path.join(save_loc,'train_and_eval_results.txt'),'a') as file:\n",
    "    file.write(\"-------XGB-------\\n\")\n",
    "    file.write(f\"Accuracy Score: \")\n",
    "    file.write(str(accuracy_score(y_test, xgb_preds)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write('Classification Report:\\n')\n",
    "    file.write(str(classification_report(y_test, xgb_preds)))\n",
    "    file.write(\"\\n\\n\\n\\n\")\n",
    "\n",
    "## DECISION TREE\n",
    "dt = DecisionTreeClassifier(max_depth = 4)\n",
    "dt.fit(X_train_smote, y_train_smote)\n",
    "dt_preds = dt.predict(X_test)\n",
    "\n",
    "print(\"-------DECISION TREE--------\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test, dt_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test, dt_preds))\n",
    "dump(dt, os.path.join(save_loc, 'dt.pkl'))\n",
    "\n",
    "with open(os.path.join(save_loc,'train_and_eval_results.txt'),'a') as file:\n",
    "    file.write(\"-------Decision Tree-------\\n\")\n",
    "    file.write(f\"Accuracy Score: \")\n",
    "    file.write(str(accuracy_score(y_test, dt_preds)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write('Classification Report:\\n')\n",
    "    file.write(str(classification_report(y_test, dt_preds)))\n",
    "    file.write(\"\\n\\n\\n\\n\")\n",
    "\n",
    "## RANDOM FOREST\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=4)\n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "rf_preds = rf.predict(X_test)\n",
    "\n",
    "print(\"-------RANDOM FOREST-------\\n\")\n",
    "print(\"ACCURACY: \", accuracy_score(y_test, rf_preds))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_test, rf_preds))\n",
    "# dump(rf, os.path.join(save_loc, 'rf.pkl'))\n",
    "\n",
    "with open(os.path.join(save_loc,'train_and_eval_results.txt'),'a') as file:\n",
    "    file.write(\"-------Random Forest-------\")\n",
    "    file.write(f\"Accuracy Score: \")\n",
    "    file.write(str(accuracy_score(y_test, rf_preds)))\n",
    "    file.write(\"\\n\")\n",
    "    file.write('Classification Report:\\n')\n",
    "    file.write(str(classification_report(y_test, rf_preds)))\n",
    "    file.write(\"\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360028b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
