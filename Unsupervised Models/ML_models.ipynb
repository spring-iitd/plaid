{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, silhouette_score\n",
    "# import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fuzzy_dataset.csv',\n",
       " 'normal_run_data.txt',\n",
       " 'gear_dataset.csv',\n",
       " '.DS_Store',\n",
       " 'RPM_dataset.csv',\n",
       " 'DoS_dataset.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../car_hacking_data/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_data_path = os.path.join(data_dir, \"normal_run_data.txt\")\n",
    "dos_data_path = os.path.join(data_dir, 'DoS_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_to_dec = lambda x: int(x, 16)\n",
    "\n",
    "## Since there are varying DLCs (2,5,6,8) in order to maintain data integrity\n",
    "## The data must be padded with 00s when DLC < 8\n",
    "\n",
    "def shift_columns(df):\n",
    "    \n",
    "    for dlc in [2,5,6]:\n",
    "\n",
    "        df.loc[df['dlc'] == dlc, df.columns[3:]] = df.loc[df['dlc'] == dlc, df.columns[3:]].shift(periods=8-dlc, axis='columns', fill_value='00')\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_attack_data(data_path):\n",
    "    \n",
    "    columns = ['timestamp','can_id', 'dlc', 'data0', 'data1', 'data2', 'data3', 'data4', \n",
    "           'data5', 'data6', 'data7', 'flag']\n",
    "    \n",
    "    data = pd.read_csv(data_path, names = columns)\n",
    "\n",
    "    data = shift_columns(data)\n",
    "    \n",
    "    ##Replacing all NaNs with '00' \n",
    "    data = data.replace(np.NaN, '00')\n",
    "    \n",
    "    ##Joining all data columns to put all data in one column\n",
    "    data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "    ##The data column is in hexadecimal\n",
    "    data['data'] = data[data_cols].apply(''.join, axis=1)\n",
    "    data.drop(columns = data_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    ##Converting columns to decimal\n",
    "    data['can_id'] = data['can_id'].apply(hex_to_dec)\n",
    "    data['data'] = data['data'].apply(hex_to_dec)\n",
    "    \n",
    "    data = data.assign(IAT=data['timestamp'].diff().fillna(0))\n",
    "    \n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "ids = []\n",
    "dlcs = []\n",
    "data = []\n",
    "\n",
    "# Read the data from the file\n",
    "with open(benign_data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Extract information from each line\n",
    "        line = line.strip()\n",
    "        ts = line.split('Timestamp: ')[1].split(' ')[0]\n",
    "        can_id = line.split('ID: ')[1].split(' ')[0]\n",
    "        dlc = line.split('DLC: ')[1].split(' ')[0]\n",
    "        can_data = ''.join(line.split('DLC: ')[1].split(' ')[1:])\n",
    "        \n",
    "        #Converting Hexadecimal entries to decimal format\n",
    "        timestamps.append(float(ts))\n",
    "        ids.append(hex_to_dec(can_id))\n",
    "        dlcs.append(int(dlc))\n",
    "        data.append(hex_to_dec(can_data))\n",
    "        \n",
    "benign_data = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'can_id': ids,\n",
    "    'dlc': dlcs,\n",
    "    'data': data\n",
    "})\n",
    "\n",
    "benign_data.sort_values(by = ['timestamp'], inplace = True)\n",
    "\n",
    "# Creating IAT column\n",
    "benign_data= benign_data.assign(IAT=benign_data['timestamp'].diff().fillna(0))\n",
    "benign_data.drop(columns = ['timestamp'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = benign_data[['can_id', 'dlc', 'data', 'IAT']].values\n",
    "X = X[:30_000]\n",
    "\n",
    "y = np.zeros(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_data = read_attack_data(dos_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_data = dos_data[dos_data['flag'] == 'T'][:20_000]\n",
    "\n",
    "X_test = dos_data[['can_id', 'dlc', 'data', 'IAT']].values\n",
    "y_test = dos_data['flag'].replace({'T' : 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(inp, reconstructions):\n",
    "\n",
    "    return np.mean(np.square(inp, reconstructions), axis = 1)\n",
    "\n",
    "def generate_predictions(inp, reconstruction, threshold):\n",
    "\n",
    "    reconstruction_error = mse(inp, reconstruction)\n",
    "    preds = reconstruction_error > threshold\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOOOST\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9a9f91f01f68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mxgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "# print(\"XGBOOOST\")\n",
    "\n",
    "# xgb_param_grid = {\n",
    "#         'max_depth' : [5, 7],\n",
    "#         'booster' : ['gbtree', 'gblinear'],\n",
    "#         'eta' : [0.3],\n",
    "#         'eval_metric' : ['rmse'],\n",
    "#         'n_estimators' : [100, 200],\n",
    "#         'subsample' : [0.8]}\n",
    "\n",
    "\n",
    "# xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "\n",
    "# print(\"Grid Searching for best model!\")\n",
    "# xgb_gs = GridSearchCV(xgb_model, param_grid=xgb_param_grid, cv = 10, scoring = 'neg_mean_squared_error', verbose = 2)\n",
    "# xgb_gs.fit(X_train, X_train)\n",
    "\n",
    "# print(\"Best Parameters:\",xgb_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_best = xgb_gs.best_estimator_\n",
    "\n",
    "# train_reconstructions = xgb_best.predict(X_train)\n",
    "# train_reconstruction_error = mse(X_train, train_reconstructions)\n",
    "\n",
    "# val_reconstructions = xgb_best.predict(X_val)\n",
    "\n",
    "# test_reconstructions = xgb_best.predict(X_test)\n",
    "\n",
    "# thresholds_to_test = np.arange(0.1, 1.0, 0.05)\n",
    "\n",
    "# for thresholds in thresholds_to_test:\n",
    "    \n",
    "#     threshold = np.quantile(train_reconstruction_error, thresholds)\n",
    "    \n",
    "#     print(\"Currently testing threshold:\", thresholds)\n",
    "\n",
    "#     val_preds = generate_predictions(X_val, val_reconstructions, threshold)\n",
    "#     print(\"Validation Accuracy:\", accuracy_score(y_val, val_preds))\n",
    "\n",
    "#     test_preds = generate_predictions(X_test, test_reconstructions, threshold)\n",
    "#     print(\"Testing Accuracy Score:\", accuracy_score(y_test, test_preds))\n",
    "\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_scaler = StandardScaler()\n",
    "X_stan = clust_scaler.fit_transform(X)\n",
    "\n",
    "anomaly_data = dos_data[['can_id', 'dlc', 'data', 'IAT']].values\n",
    "anomaly_data = clust_scaler.transform(anomaly_data)\n",
    "validation_data = anomaly_data[:250]\n",
    "test_data = anomaly_data[250:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine X_stan and validation_data for training\n",
    "combined_data = np.vstack((X_stan, validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "print(len(X_stan))\n",
    "print(len(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----SVM------\n",
      "Testing combination 1 of 20\n"
     ]
    }
   ],
   "source": [
    "print(\"-----SVM------\")\n",
    "\n",
    "\n",
    "svm_param_grid = {\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'nu': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "\n",
    "total_combinations = len(svm_param_grid['kernel']) * len(svm_param_grid['nu']) * len(svm_param_grid['gamma'])\n",
    "count = 0\n",
    "\n",
    "# Iterate over the parameter grid\n",
    "for kernel in svm_param_grid['kernel']:\n",
    "    for nu in svm_param_grid['nu']:\n",
    "        for gamma in svm_param_grid['gamma']:\n",
    "        \n",
    "            count += 1\n",
    "        \n",
    "            print(f\"Testing combination {count} of {total_combinations}\")\n",
    "            # Create an instance of OneClassSVM with current parameter combination\n",
    "            svm = OneClassSVM(kernel=kernel, nu=nu, gamma=gamma)\n",
    "        \n",
    "            # Fit OneClassSVM on training data\n",
    "            svm.fit(X_train)\n",
    "        \n",
    "            # Evaluate performance using a suitable metric (e.g., accuracy score)\n",
    "            validation_preds = svm.predict(X_val)\n",
    "            validation_preds[validation_preds == -1] = 0\n",
    "            score = accuracy_score(y_val, validation_preds)\n",
    "        \n",
    "            # Update best parameters if current score is higher\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = {'kernel': kernel, 'nu': nu, 'gamma': gamma}\n",
    "\n",
    "print(\"Best params:\", best_params)\n",
    "\n",
    "# Get the best OneClassSVM model and its parameters\n",
    "best_svm = OneClassSVM(kernel=best_params['kernel'], nu=best_params['nu'], gamma=best_params['gamma'])\n",
    "best_svm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-0bcb57dd9327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_svm' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Use the best OneClassSVM model for anomaly detection on validation data\n",
    "# val_preds = best_svm.fit_predict(X_val)\n",
    "# val_preds[val_preds == -1] = 1\n",
    "\n",
    "\n",
    "# # Use the best OneClassSVM model for anomaly detection on test data\n",
    "# test_preds = best_svm.predict(X_test)\n",
    "# test_preds[test_preds == -1] = 1\n",
    "\n",
    "# print(\"Validation Accuracy:\",accuracy_score(y_val,  val_preds))\n",
    "# print(\"Testing Accuracy:\",accuracy_score(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = OneClassSVM()\n",
    "\n",
    "# Perform hyperparameter search using GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, scoring='f1')\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_scaled)\n",
    "\n",
    "# Get the best hyperparameters and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Fit the best model to the data\n",
    "best_model.fit(X_scaled)\n",
    "\n",
    "# Predict on the data\n",
    "predictions = best_model.predict(X_scaled)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, predictions)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"F1 Score:\", f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
