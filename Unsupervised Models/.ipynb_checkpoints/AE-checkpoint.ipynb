{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beeec2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 17:04:32.909521: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, MultiHeadAttention, LayerNormalization, RepeatVector, LeakyReLU, Flatten, TimeDistributed, add\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6ab8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 17:04:51.315190: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-07-31 17:04:51.343853: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/soft/centOS/lib/gnu/tcl/8.4.20/lib\n",
      "2023-07-31 17:04:51.352828: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-07-31 17:04:51.353491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (chas181): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d97149f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fuzzy_dataset.csv',\n",
       " 'normal_run_data.txt',\n",
       " 'gear_dataset.csv',\n",
       " '.DS_Store',\n",
       " 'RPM_dataset.csv',\n",
       " 'DoS_dataset.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../car_hacking_data/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1bb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_data_path = os.path.join(data_dir, \"normal_run_data.txt\")\n",
    "dos_data_path = os.path.join(data_dir, 'DoS_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d187bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_to_dec = lambda x: int(x, 16)\n",
    "\n",
    "## Since there are varying DLCs (2,5,6,8) in order to maintain data integrity\n",
    "## The data must be padded with 00s when DLC < 8\n",
    "\n",
    "def shift_columns(df):\n",
    "    \n",
    "    for dlc in [2,5,6]:\n",
    "\n",
    "        df.loc[df['dlc'] == dlc, df.columns[3:]] = df.loc[df['dlc'] == dlc, df.columns[3:]].shift(periods=8-dlc, axis='columns', fill_value='00')\n",
    "\n",
    "    return df\n",
    "\n",
    "def pad_with_zeros(string, desired_length=16):\n",
    "    if len(string) >= desired_length:\n",
    "        return string\n",
    "    else:\n",
    "        return string.zfill(desired_length)\n",
    "    \n",
    "def split_string_into_list(string):\n",
    "    # Initialize an empty list to store the result\n",
    "    result_list = []\n",
    "\n",
    "    # Iterate through the string with a step size of 2\n",
    "    for i in range(0, len(string), 2):\n",
    "        # Extract two characters at a time and add them to the result list\n",
    "        item = string[i:i+2]\n",
    "        result_list.append(item)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9cf9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_attack_data(data_path):\n",
    "    \n",
    "    columns = ['timestamp','can_id', 'dlc', 'data0', 'data1', 'data2', 'data3', 'data4', \n",
    "           'data5', 'data6', 'data7', 'flag']\n",
    "    \n",
    "    data = pd.read_csv(data_path, names = columns)\n",
    "\n",
    "    data = shift_columns(data)\n",
    "    \n",
    "    ##Replacing all NaNs with '00' \n",
    "    data = data.replace(np.NaN, '00')\n",
    "    \n",
    "    ##Joining all data columns to put all data in one column\n",
    "    data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "    ##The data column is in hexadecimal\n",
    "#     data['data'] = data[data_cols].apply(''.join, axis=1)\n",
    "#     data.drop(columns = data_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    ##Converting columns to decimal\n",
    "    data['can_id'] = data['can_id'].apply(hex_to_dec)\n",
    "    data[data_cols] = data[data_cols].astype(str)\n",
    "    \n",
    "    data.sort_values(by = ['timestamp'], inplace = True)\n",
    "    data = data.assign(IAT=data['timestamp'].diff().fillna(0))\n",
    "    data.drop(['timestamp'], inplace = True, axis = 1)\n",
    "    \n",
    "    data[data_cols] = data[data_cols].applymap(hex_to_dec)\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e940bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "ids = []\n",
    "dlcs = []\n",
    "data = []\n",
    "data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "# Read the data from the file\n",
    "with open(benign_data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Extract information from each line\n",
    "        line = line.strip()\n",
    "        ts = line.split('Timestamp: ')[1].split(' ')[0]\n",
    "        can_id = line.split('ID: ')[1].split(' ')[0]\n",
    "        dlc = line.split('DLC: ')[1].split(' ')[0]\n",
    "        can_data = ''.join(line.split('DLC: ')[1].split(' ')[1:])\n",
    "        \n",
    "        can_data = pad_with_zeros(can_data)\n",
    "        data_split = split_string_into_list(can_data)\n",
    "               \n",
    "        #Converting Hexadecimal entries to decimal format\n",
    "        timestamps.append(float(ts))\n",
    "        ids.append(hex_to_dec(can_id))\n",
    "        dlcs.append(int(dlc))\n",
    "        data.append([hex_to_dec(hex_str) for hex_str in data_split])\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "# data_dict = {f\"data{i}\": col for i, col in enumerate(data_split)}\n",
    "        \n",
    "benign = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'can_id': ids,\n",
    "    'dlc': dlcs})\n",
    "\n",
    "data = pd.DataFrame(data, columns = data_cols)\n",
    "\n",
    "benign_data = pd.concat([benign, data], axis=1)\n",
    "benign_data.sort_values(by = ['timestamp'], inplace = True)\n",
    "\n",
    "# # Creating IAT column\n",
    "benign_data= benign_data.assign(IAT=benign_data['timestamp'].diff().fillna(0))\n",
    "benign_data.drop(columns = ['timestamp'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c49417e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['can_id', 'dlc', 'data0', 'data1', 'data2', 'data3', 'data4', 'data5',\n",
      "       'data6', 'data7', 'IAT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(benign_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5ea56c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'can_id', 'dlc', 'data0', 'data1', 'data2', 'data3',\n",
      "       'data4', 'data5', 'data6', 'data7', 'flag', 'IAT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "test = read_attack_data(dos_data_path)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "37a4fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = benign_data.values\n",
    "\n",
    "test = read_attack_data(dos_data_path)\n",
    "x_test = test.drop(['flag'], axis = 1)\n",
    "y_test = test['flag'].replace({'R' : 0, 'T' : 1})\n",
    "\n",
    "x_test = x_test.values\n",
    "\n",
    "val_idx = int(0.8 * len(X))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = X[:val_idx]\n",
    "X_val = X[val_idx:]\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c584b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791096, 11)\n",
      "(197775, 11)\n",
      "(3665771, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "127c84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a sequencified dataset for LSTM moodel\n",
    "def sequencify(dataset, start, end, window):\n",
    "  \n",
    "    X = []\n",
    "    \n",
    "    start = start + window \n",
    "    if end is None:\n",
    "        end = len(dataset)\n",
    "        \n",
    "    for i in range(start, end+1):\n",
    "        indices = range(i-window, i) \n",
    "        X.append(dataset[indices])\n",
    "\t\t\t\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3df6df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2818e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = sequencify(X_train, 0, None, seq_size)\n",
    "X_val_seq = sequencify(X_val, 0, None, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2391363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon # returns mean plus std dev x (random) epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5affa051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791087, 10, 11)\n",
      "(197766, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq.shape)\n",
    "print(X_val_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7f294077",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change loss fn, figure out issue related to shape\n",
    "\n",
    "def make_AE(latent_dim = 256, input_shape = (10, 11), num_heads = 8, key_dim = 16, seq_size = seq_size):\n",
    "    \n",
    "    features = input_shape[-1]\n",
    "    \n",
    "    inp = Input(shape = input_shape, name = 'encoder_inp')\n",
    "    \n",
    "    # Create the MultiHeadAttention layer\n",
    "    \n",
    "    encoder_attention = attention_block(inputs = inp, num_heads = num_heads, head_size = key_dim, ff_dim = features + 2\n",
    "                                      ,dropout = 0.3)\n",
    "\n",
    "    \n",
    "#     x = TimeDistributed(Dense((256), name = 'encoder_dense_3'))(x)\n",
    "#     x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "#     x = TimeDistributed(Dense((128), name = 'encoder_dense_4'))(x)\n",
    "#     x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features), name = 'encoder_dense_1')(encoder_attention)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "   \n",
    "    x1 = TimeDistributed(Dense(features - 1), name = 'encoder_dense_2')(x)\n",
    "    x1 = LeakyReLU(alpha = 0.2)(x1)\n",
    "    \n",
    "    x2 = TimeDistributed(Dense(features - 2), name = 'encoder_dense_3')(x1)\n",
    "    x2 = LeakyReLU(alpha = 0.2)(x2)\n",
    "    \n",
    "\n",
    "    \n",
    "    flattened_output = Flatten(name = 'encoder_flatten')(x2)\n",
    "    \n",
    "#     z_mean = layers.Dense(latent_dim, name=\"z_mean\")(flattened_output)\n",
    "#     z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(flattened_output)\n",
    "    \n",
    "#     z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    code_layer = Dense(latent_dim, name = 'code')(flattened_output)\n",
    "    \n",
    "#     encoder_vae = Model(inputs=inp,\n",
    "#                             outputs=[z_mean, z_log_var, z],\n",
    "#                             name='Attention_VAE_encoder')\n",
    "    \n",
    "    encoder_ae = Model(inputs=inp,\n",
    "                        outputs=code_layer,\n",
    "                        name='Attention_AE_encoder')\n",
    "    \n",
    "    inp_decoder = Input(shape = (latent_dim,), name = 'decoder_inp')\n",
    "    \n",
    "    repeat_vec = RepeatVector(seq_size, name = 'repeat_vec')(inp_decoder)\n",
    "    \n",
    "    decoder_attention = attention_block(inputs = repeat_vec, num_heads = num_heads, head_size = key_dim, ff_dim = features + 2\n",
    "                                      ,dropout = 0.3)\n",
    "#     attention_output = decoder_attention_layer(repeat_vec, repeat_vec)\n",
    "#     attention_output = LayerNormalization(epsilon=1e-6, name = 'decoder_norm')(attention_output)\n",
    "    \n",
    "#     flattened_output = Flatten(name = 'decoder_flatten')(attention_output)\n",
    "    \n",
    "    y = TimeDistributed(Dense(features - 2), name = 'decoder_dense_1')(decoder_attention)\n",
    "    y = LeakyReLU(alpha = 0.2)(y)\n",
    "    \n",
    "#     res1 = add([x2, y])\n",
    "    \n",
    "    y1 = TimeDistributed(Dense(features - 2), name = 'decoder_dense_2')(y)\n",
    "    y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    \n",
    "#     res2 = add([x1,y1])\n",
    "    \n",
    "    y2 = TimeDistributed(Dense(features - 1), name = 'decoder_dense_3')(y1)\n",
    "    y2 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    \n",
    "    res3 = add([x2,y2])\n",
    "    \n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(input_shape[-1], activation='linear', name = 'decoder_op'))(res3)\n",
    "    output = add([x2,y2])\n",
    "    \n",
    "    decoder_ae = Model(inputs=inp_decoder, outputs=output, name='Attention_AE_decoder')\n",
    "\n",
    "    # VAE model\n",
    "    ae_inputs = inp\n",
    "    z = encoder_ae(ae_inputs)\n",
    "    ae_outputs = decoder_ae(z)\n",
    "    ae = Model(inputs=ae_inputs, outputs=ae_outputs, name='Attention_AE')\n",
    "\n",
    "    return encoder_ae, decoder_ae, ae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7332ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Attention_AE_decoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " decoder_inp (InputLayer)       [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " repeat_vec (RepeatVector)      (None, 15, 256)      0           ['decoder_inp[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 15, 256)     512         ['repeat_vec[0][0]']             \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 15, 256)     131712      ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 15, 256)      0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 15, 256)     0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'repeat_vec[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 15, 256)     512         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 15, 6)        1542        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 15, 6)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 15, 256)      1792        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 15, 256)     0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDistri  (None, 15, 2)       514         ['tf.__operators__.add_3[0][0]'] \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 15, 2)        0           ['time_distributed_3[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_4 (TimeDistri  (None, 15, 2)       6           ['leaky_re_lu_3[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)      (None, 15, 2)        0           ['time_distributed_4[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_5 (TimeDistri  (None, 15, 3)       9           ['leaky_re_lu_4[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)      (None, 15, 3)        0           ['time_distributed_5[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_6 (TimeDistri  (None, 15, 3)       12          ['leaky_re_lu_5[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 15, 3)        0           ['time_distributed_6[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_7 (TimeDistri  (None, 15, 4)       16          ['leaky_re_lu_6[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 136,627\n",
      "Trainable params: 136,627\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_block():\n",
    "\n",
    "\n",
    "def lstm_ae():\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b1a2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(data, reconstruction):\n",
    "    mu, ln_var, z = encoder_vae(data)\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = tf.reduce_mean(\n",
    "        tf.keras.losses.mean_squared_error(data, reconstruction)\n",
    "    ) \n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * tf.reduce_sum(1 + ln_var - tf.square(mu) - tf.exp(ln_var), axis=-1)\n",
    "    kl_loss = tf.reduce_mean(kl_loss)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f776d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d0c31ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 15, 4), dtype=tf.float32, name='encoder_inp'), name='encoder_inp', description=\"created by layer 'encoder_inp'\") at layer \"layer_normalization_8\". The following previous layers were accessed without issue: ['repeat_vec']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m strat \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mMirroredStrategy()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strat\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m----> 4\u001b[0m     encoder_ae, decoder_ae, ae \u001b[38;5;241m=\u001b[39m \u001b[43mmake_AE\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     ae\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 79\u001b[0m, in \u001b[0;36mmake_AE\u001b[0;34m(latent_dim, input_shape, num_heads, key_dim, seq_size)\u001b[0m\n\u001b[1;32m     76\u001b[0m output \u001b[38;5;241m=\u001b[39m TimeDistributed(Dense(input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_op\u001b[39m\u001b[38;5;124m'\u001b[39m))(res3)\n\u001b[1;32m     77\u001b[0m output \u001b[38;5;241m=\u001b[39m add([x2,y2])\n\u001b[0;32m---> 79\u001b[0m decoder_ae \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAttention_AE_decoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# VAE model\u001b[39;00m\n\u001b[1;32m     82\u001b[0m ae_inputs \u001b[38;5;241m=\u001b[39m inp\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/engine/functional.py:148\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, trainable, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([functional_utils\u001b[38;5;241m.\u001b[39mis_input_keras_tensor(t)\n\u001b[1;32m    146\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(inputs)]):\n\u001b[1;32m    147\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m functional_utils\u001b[38;5;241m.\u001b[39mclone_graph_nodes(inputs, outputs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py:587\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 587\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    589\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/engine/functional.py:232\u001b[0m, in \u001b[0;36mFunctional._init_graph_network\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_coordinates\u001b[38;5;241m.\u001b[39mappend((layer, node_index, tensor_index))\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Keep track of the network's nodes and layers.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m nodes, nodes_by_depth, layers, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_map_graph_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/engine/functional.py:998\u001b[0m, in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39mkeras_inputs):\n\u001b[1;32m    997\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(x) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m computable_tensors:\n\u001b[0;32m--> 998\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph disconnected: cannot obtain value for tensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. The following previous layers \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwere accessed without issue: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayers_with_complete_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(node\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[1;32m   1003\u001b[0m   computable_tensors\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(x))\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 15, 4), dtype=tf.float32, name='encoder_inp'), name='encoder_inp', description=\"created by layer 'encoder_inp'\") at layer \"layer_normalization_8\". The following previous layers were accessed without issue: ['repeat_vec']"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strat.scope():\n",
    "    encoder_ae, decoder_ae, ae = make_AE()\n",
    "    ae.compile(loss = 'mae', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ece6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.time()\n",
    "datetime_obj = datetime.fromtimestamp(timestamp)\n",
    "fmt_time = datetime_obj.strftime('%m-%d %H:%M:%S')\n",
    "\n",
    "tb = TensorBoard(log_dir=f'vae_logs/{fmt_time}')\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 15, restore_best_weights=True)\n",
    "\n",
    "ckpt = ModelCheckpoint(filepath = 'vae_cpkts/model-{epoch:02d}-{val_loss:.4f}.hdf5',\n",
    "                      monitor = 'val_loss',\n",
    "                      mode = 'min',\n",
    "                      save_best_only = True,\n",
    "                      verbose = 1)\n",
    "\n",
    "red_lr = ReduceLROnPlateau(patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "623f592c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Attention_AE_encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inp (InputLayer)       [(None, 15, 4)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 15, 4)       8           ['encoder_inp[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 15, 4)       2436        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 15, 4)        0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 15, 4)       0           ['dropout[0][0]',                \n",
      " da)                                                              'encoder_inp[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 15, 4)       8           ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 15, 6)        30          ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 15, 6)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 15, 4)        28          ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 15, 4)       0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 15, 4)       20          ['tf.__operators__.add_1[0][0]'] \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 15, 4)        0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDistri  (None, 15, 3)       15          ['leaky_re_lu[0][0]']            \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 15, 3)        0           ['time_distributed_1[0][0]']     \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDistri  (None, 15, 2)       8           ['leaky_re_lu_1[0][0]']          \n",
      " buted)                                                                                           \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 15, 2)        0           ['time_distributed_2[0][0]']     \n",
      "                                                                                                  \n",
      " encoder_flatten (Flatten)      (None, 30)           0           ['leaky_re_lu_2[0][0]']          \n",
      "                                                                                                  \n",
      " code (Dense)                   (None, 256)          7936        ['encoder_flatten[0][0]']        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 10,489\n",
      "Trainable params: 10,489\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_ae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92e720da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:29:21.238623: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1415\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:4\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2023-07-24 04:29:21.358430: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:34:47.093381: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_103521\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:32\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2023-07-24 04:34:47.152373: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.98015, saving model to vae_cpkts/model-01-0.9802.hdf5\n",
      "24722/24722 [==============================] - 360s 14ms/step - loss: 0.9878 - val_loss: 0.9802 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "24719/24722 [============================>.] - ETA: 0s - loss: 0.9870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:40:40.578282: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.98015 to 0.97980, saving model to vae_cpkts/model-02-0.9798.hdf5\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9870 - val_loss: 0.9798 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "24720/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:46:33.175701: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss did not improve from 0.97980\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9868 - val_loss: 0.9798 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:52:26.042157: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.97980 to 0.97967, saving model to vae_cpkts/model-04-0.9797.hdf5\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:58:18.410601: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.97967 to 0.97965, saving model to vae_cpkts/model-05-0.9797.hdf5\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:04:11.228270: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss did not improve from 0.97965\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:10:04.137678: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.97965 to 0.97965, saving model to vae_cpkts/model-07-0.9797.hdf5\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:15:56.585162: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss did not improve from 0.97965\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:21:49.378092: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 0.97965 to 0.97961, saving model to vae_cpkts/model-09-0.9796.hdf5\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9867 - val_loss: 0.9796 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "24720/24722 [============================>.] - ETA: 0s - loss: 0.9867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:27:43.166422: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss did not improve from 0.97961\n",
      "24722/24722 [==============================] - 354s 14ms/step - loss: 0.9867 - val_loss: 0.9796 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "21081/24722 [========================>.....] - ETA: 47s - loss: 0.9868"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m strat\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mred_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256 * strat.num_replicas_in_sync\n",
    "\n",
    "history = ae.fit(X_train_seq, X_train_seq, validation_data = (X_val_seq, X_val_seq), callbacks = [tb, es, ckpt, red_lr], \n",
    "         epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change loss fn, figure out issue related to shape\n",
    "\n",
    "def make_VAE(latent_dim = 256, input_shape = (15, 4), num_heads = 16, key_dim = 4, seq_size = seq_size):\n",
    "    \n",
    "    features = input_shape[-1]\n",
    "    \n",
    "    inp = Input(shape = input_shape, name = 'encoder_inp')\n",
    "    \n",
    "    # Create the MultiHeadAttention layer\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, name = 'encoder_attention')\n",
    "    \n",
    "    attention_output = attention_layer(inp, inp)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6, name = 'encoder_norm')(attention_output)\n",
    "    \n",
    "#     flattened_output = Flatten(name = 'encoder_flatten')(attention_output)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features, name = 'encoder_dense_1'))(attention_output)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features, name = 'encoder_dense_2'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "   \n",
    "    x = TimeDistributed(Dense(features - 1, name = 'encoder_dense_3'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 1, name = 'encoder_dense_4'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 2, name = 'encoder_dense_5'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 2, name = 'encoder_dense_6'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    flattened_output = Flatten(name = 'encoder_flatten')(x)\n",
    "    \n",
    "#     z_mean = layers.Dense(latent_dim, name=\"z_mean\")(flattened_output)\n",
    "#     z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(flattened_output)\n",
    "    \n",
    "#     z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    code_layer = Dense(latent_dim, name = 'code')(flattened_output)\n",
    "    \n",
    "#     encoder_vae = Model(inputs=inp,\n",
    "#                             outputs=[z_mean, z_log_var, z],\n",
    "#                             name='Attention_VAE_encoder')\n",
    "    \n",
    "    encoder_vae = Model(inputs=inp,\n",
    "                        outputs=code_layer,\n",
    "                        name='Attention_VAE_encoder')\n",
    "    \n",
    "    inp_decoder = Input(shape = (latent_dim,), name = 'decoder_inp')\n",
    "    \n",
    "    decoder_attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, name = 'decoder_attention')\n",
    "    \n",
    "    repeat_vec = RepeatVector(seq_size, name = 'repeat_vec')(inp_decoder)\n",
    "    attention_output = decoder_attention_layer(repeat_vec, repeat_vec)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6, name = 'decoder_norm')(attention_output)\n",
    "    \n",
    "#     flattened_output = Flatten(name = 'decoder_flatten')(attention_output)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 2, name = 'decoder_dense_1'))(attention_output)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 2, name = 'decoder_dense_2'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 1, name = 'decoder_dense_3'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 1, name = 'decoder_dense_4'))(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(input_shape[-1], activation='linear', name = 'decoder_op'))(x)\n",
    "\n",
    "    decoder_vae = Model(inputs=inp_decoder, outputs=output, name='Attention_VAE_decoder')\n",
    "\n",
    "    # VAE model\n",
    "    vae_inputs = inp\n",
    "    z = encoder_vae(vae_inputs)\n",
    "    vae_outputs = decoder_vae(z)\n",
    "    vae = Model(inputs=vae_inputs, outputs=vae_outputs, name='Attention_VAE')\n",
    "\n",
    "    return encoder_vae, decoder_vae, vae"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
