{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beeec2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 17:26:28.974858: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, MultiHeadAttention, LayerNormalization, RepeatVector, LeakyReLU, Flatten, TimeDistributed, Add, Conv1D, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6ab8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-11 17:28:35.996115: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-03-11 17:28:36.029002: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2024-03-11 17:28:36.203390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:02:00.0 name: Tesla K40m computeCapability: 3.5\n",
      "coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s\n",
      "2024-03-11 17:28:36.204550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:84:00.0 name: Tesla K40m computeCapability: 3.5\n",
      "coreClock: 0.745GHz coreCount: 15 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 268.58GiB/s\n",
      "2024-03-11 17:28:36.204593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2024-03-11 17:28:39.940106: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2024-03-11 17:28:39.940191: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2024-03-11 17:28:40.777549: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2024-03-11 17:28:41.253055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2024-03-11 17:28:42.281666: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2024-03-11 17:28:42.720176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2024-03-11 17:28:44.239671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2024-03-11 17:28:44.243736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97149f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lstm.h5',\n",
       " 'DoS_dataset.csv',\n",
       " 'mlp_training_history.png',\n",
       " '.ipynb_checkpoints',\n",
       " 'evaluation_results.txt',\n",
       " 'Car Hacking Dataset- DoS vs Robust Weights (MLP - Layer 1, 2nd Feature).png',\n",
       " 'exp_factor_0.08',\n",
       " 'exp_factor_0.05',\n",
       " 'exp_factor_0.01',\n",
       " 'Car Hacking Dataset- DoS vs Robust Weights (MLP - Layer 1, 4th Feature).png',\n",
       " 'exp_factor_0.09',\n",
       " 'Car Hacking Dataset- DoS vs Robust Weights (MLP - Layer 1, 3rd Feature).png',\n",
       " 'exp_factor_0.07',\n",
       " 'exp_factor_0.02',\n",
       " 'exp_factor_0.075',\n",
       " 'exp_factor_0.055',\n",
       " 'exp_factor_0.065',\n",
       " 'exp_factor_0.045',\n",
       " 'exp_factor_0.095',\n",
       " 'dt.pkl',\n",
       " 'Car Hacking Dataset- DoS vs Robust Weights (MLP - Layer 1, 1st Feature).png',\n",
       " 'smart_output.csv',\n",
       " 'exp_factor_0.06',\n",
       " 'Adversarial Training Evaluation',\n",
       " 'rf.pkl',\n",
       " 'Traditional DoS Training and Testing',\n",
       " 'scaler.joblib',\n",
       " 'exp_factor_0.085',\n",
       " 'exp_factor_0.015',\n",
       " 'xgb.json',\n",
       " 'exp_factor_0.1',\n",
       " 'lstm_training_history.png',\n",
       " 'DoS-to-Adversarial Transfer Learning Evaluation',\n",
       " 'seq_scaler.joblib',\n",
       " 'MLP_weights_plot.png',\n",
       " 'preprocessed_car_hacking.csv',\n",
       " 'exp_factor_0.035',\n",
       " 'eval_on_smart_attack.txt',\n",
       " '.DS_Store',\n",
       " 'Car Hacking Dataset- DoS vs Robust Weights (MLP - Layer 3).png',\n",
       " 'mlp.h5',\n",
       " 'normal_run_data.txt',\n",
       " 'benign_data.csv',\n",
       " 'exp_factor_0.04']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../Smart Attack Algorithm/data/Car Hacking Dataset/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1bb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_data_path = os.path.join(data_dir, \"benign_data.csv\")\n",
    "# dos_data_path = os.path.join(data_dir, 'DoS_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d187bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_to_dec = lambda x: int(x, 16)\n",
    "\n",
    "## Since there are varying DLCs (2,5,6,8) in order to maintain data integrity\n",
    "## The data must be padded with 00s when DLC < 8\n",
    "\n",
    "def shift_columns(df):\n",
    "    \n",
    "    for dlc in [2,5,6]:\n",
    "\n",
    "        df.loc[df['dlc'] == dlc, df.columns[3:]] = df.loc[df['dlc'] == dlc, df.columns[3:]].shift(periods=8-dlc, axis='columns', fill_value='00')\n",
    "\n",
    "    return df\n",
    "\n",
    "def pad_with_zeros(string, desired_length=16):\n",
    "    if len(string) >= desired_length:\n",
    "        return string\n",
    "    else:\n",
    "        return string.zfill(desired_length)\n",
    "    \n",
    "def split_string_into_list(string):\n",
    "    # Initialize an empty list to store the result\n",
    "    result_list = []\n",
    "\n",
    "    # Iterate through the string with a step size of 2\n",
    "    for i in range(0, len(string), 2):\n",
    "        # Extract two characters at a time and add them to the result list\n",
    "        item = string[i:i+2]\n",
    "        result_list.append(item)\n",
    "\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9cf9b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_attack_data(data_path):\n",
    "    \n",
    "    columns = ['timestamp','can_id', 'dlc', 'data0', 'data1', 'data2', 'data3', 'data4', \n",
    "           'data5', 'data6', 'data7', 'flag']\n",
    "    \n",
    "    data = pd.read_csv(data_path, names = columns)\n",
    "\n",
    "    data = shift_columns(data)\n",
    "    \n",
    "    ##Replacing all NaNs with '00' \n",
    "    data = data.replace(np.NaN, '00')\n",
    "    \n",
    "    ##Joining all data columns to put all data in one column\n",
    "    data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "    ##The data column is in hexadecimal\n",
    "#     data['data'] = data[data_cols].apply(''.join, axis=1)\n",
    "#     data.drop(columns = data_cols, inplace = True, axis = 1)\n",
    "    \n",
    "    ##Converting columns to decimal\n",
    "    data['can_id'] = data['can_id'].apply(hex_to_dec)\n",
    "    data[data_cols] = data[data_cols].astype(str)\n",
    "    \n",
    "    data.sort_values(by = ['timestamp'], inplace = True)\n",
    "    data = data.assign(IAT=data['timestamp'].diff().fillna(0))\n",
    "    data.drop(['timestamp'], inplace = True, axis = 1)\n",
    "    \n",
    "    data[data_cols] = data[data_cols].applymap(hex_to_dec)\n",
    "    \n",
    "\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e940bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = []\n",
    "ids = []\n",
    "dlcs = []\n",
    "data = []\n",
    "data_cols = ['data0', 'data1', 'data2', 'data3', 'data4', 'data5', 'data6', 'data7']\n",
    "    \n",
    "# Read the data from the file\n",
    "with open(benign_data_path, 'r') as file:\n",
    "    for line in file:\n",
    "        # Extract information from each line\n",
    "        line = line.strip()\n",
    "        ts = line.split('Timestamp: ')[1].split(' ')[0]\n",
    "        can_id = line.split('ID: ')[1].split(' ')[0]\n",
    "        dlc = line.split('DLC: ')[1].split(' ')[0]\n",
    "        can_data = ''.join(line.split('DLC: ')[1].split(' ')[1:])\n",
    "        \n",
    "        can_data = pad_with_zeros(can_data)\n",
    "        data_split = split_string_into_list(can_data)\n",
    "               \n",
    "        #Converting Hexadecimal entries to decimal format\n",
    "        timestamps.append(float(ts))\n",
    "        ids.append(hex_to_dec(can_id))\n",
    "        dlcs.append(int(dlc))\n",
    "        data.append([hex_to_dec(hex_str) for hex_str in data_split])\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "# data_dict = {f\"data{i}\": col for i, col in enumerate(data_split)}\n",
    "        \n",
    "benign = pd.DataFrame({\n",
    "    'timestamp': timestamps,\n",
    "    'can_id': ids,\n",
    "    'dlc': dlcs})\n",
    "\n",
    "data = pd.DataFrame(data, columns = data_cols)\n",
    "\n",
    "benign_data = pd.concat([benign, data], axis=1)\n",
    "benign_data.sort_values(by = ['timestamp'], inplace = True)\n",
    "\n",
    "# # Creating IAT column\n",
    "benign_data= benign_data.assign(IAT=benign_data['timestamp'].diff().fillna(0))\n",
    "benign_data.drop(columns = ['timestamp'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a4fcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = benign_data.values\n",
    "\n",
    "test = read_attack_data(dos_data_path)\n",
    "x_test = test.drop(['flag'], axis = 1)\n",
    "y_test = test['flag'].replace({'R' : 0, 'T' : 1})\n",
    "\n",
    "x_test = x_test.values\n",
    "\n",
    "val_idx = int(0.8 * len(X))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = X[:val_idx]\n",
    "X_val = X[val_idx:]\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c584b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791096, 11)\n",
      "(197775, 11)\n",
      "(3665771, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "127c84cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create a sequencified dataset for LSTM moodel\n",
    "def sequencify(dataset, start, end, window):\n",
    "  \n",
    "    X = []\n",
    "    \n",
    "    start = start + window \n",
    "    if end is None:\n",
    "        end = len(dataset)\n",
    "        \n",
    "    for i in range(start, end+1):\n",
    "        indices = range(i-window, i) \n",
    "        X.append(dataset[indices])\n",
    "\t\t\t\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df6df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2818e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = sequencify(X_train, 0, None, seq_size)\n",
    "X_val_seq = sequencify(X_val, 0, None, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2391363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon # returns mean plus std dev x (random) epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5affa051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791087, 10, 11)\n",
      "(197766, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq.shape)\n",
    "print(X_val_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7720c64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerBlock(inputs, num_heads, key_dim, ff_dim, dropout=0.1):\n",
    "    multihead_attention = MultiHeadAttention(key_dim=key_dim, num_heads=num_heads)\n",
    "    attention_output = multihead_attention(inputs, inputs, inputs)\n",
    "    attention_output = Dropout(dropout)(attention_output)\n",
    "    x = Add()([inputs, attention_output])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    ffn_output = Dense(ff_dim, activation='relu')(x)\n",
    "    ffn_output = Dense(x.shape[-1])(ffn_output)\n",
    "    ffn_output = Dropout(dropout)(ffn_output)\n",
    "    x = Add()([x, ffn_output])\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7f294077",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change loss fn, figure out issue related to shape\n",
    "\n",
    "def make_AE(latent_dim = 256, input_shape = (10, 11), num_heads = 8, key_dim = 16, num_blocks = 4, \n",
    "            seq_size = seq_size):\n",
    "    \n",
    "    features = input_shape[-1]\n",
    "    \n",
    "    inp = Input(shape = input_shape, name = 'encoder_inp')\n",
    "    \n",
    "    # Create the MultiHeadAttention layer\n",
    "    x = inp\n",
    "    \n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        x = TransformerBlock(x, num_heads = num_heads, key_dim = key_dim, ff_dim = 64)\n",
    "        \n",
    "#     x = TimeDistributed(Dense((256), name = 'encoder_dense_3'))(x)\n",
    "#     x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "#     x = TimeDistributed(Dense((128), name = 'encoder_dense_4'))(x)\n",
    "#     x = LeakyReLU(alpha = 0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(features - 2), name = 'encoder_dense_1')(x)\n",
    "    x = LeakyReLU(alpha = 0.2)(x)\n",
    "   \n",
    "    x1 = TimeDistributed(Dense(features - 4), name = 'encoder_dense_2')(x)\n",
    "    x1 = LeakyReLU(alpha = 0.2)(x1)\n",
    "    \n",
    "    x2 = TimeDistributed(Dense(features - 6), name = 'encoder_dense_3')(x1)\n",
    "    x2 = LeakyReLU(alpha = 0.2)(x2)\n",
    "    \n",
    "    x3 = TimeDistributed(Dense(features - 7), name = 'encoder_dense_4')(x2)\n",
    "    x3 = LeakyReLU(alpha = 0.2)(x3)\n",
    "    \n",
    "    flattened_output = Flatten(name = 'encoder_flatten')(x2)\n",
    "    \n",
    "#     z_mean = layers.Dense(latent_dim, name=\"z_mean\")(flattened_output)\n",
    "#     z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(flattened_output)\n",
    "    \n",
    "#     z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    code_layer = Dense(latent_dim, name = 'code')(flattened_output)\n",
    "    \n",
    "    encoder_ae = Model(inputs=inp,\n",
    "                        outputs=code_layer,\n",
    "                        name='Attention_AE_encoder')\n",
    "    \n",
    "    inp_decoder = Input(shape = (latent_dim,), name = 'decoder_inp')\n",
    "    \n",
    "    repeat_vec = RepeatVector(seq_size, name = 'repeat_vec')(inp_decoder)\n",
    "    \n",
    "    y = repeat_vec\n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        y = TransformerBlock(y, num_heads, key_dim, ff_dim = 64)\n",
    "    \n",
    "    y = TimeDistributed(Dense(features - 2), name = 'decoder_dense_1')(y)\n",
    "    y = LeakyReLU(alpha = 0.2)(y)\n",
    "    \n",
    "#     res1 = add([x2, y])\n",
    "    \n",
    "    y1 = TimeDistributed(Dense(features - 2), name = 'decoder_dense_2')(y)\n",
    "    y1 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    \n",
    "#     res2 = add([x1,y1])\n",
    "    \n",
    "    y2 = TimeDistributed(Dense(features - 1), name = 'decoder_dense_3')(y1)\n",
    "    y2 = LeakyReLU(alpha = 0.2)(y1)\n",
    "    \n",
    "    # Output layer\n",
    "    output = TimeDistributed(Dense(input_shape[-1], activation='linear', name = 'decoder_op'))(y2)\n",
    "    \n",
    "    decoder_ae = Model(inputs=inp_decoder, outputs=output, name='Attention_AE_decoder')\n",
    "\n",
    "    # AE model\n",
    "    ae_inputs = inp\n",
    "    z = encoder_ae(ae_inputs)\n",
    "    ae_outputs = decoder_ae(z)\n",
    "    ae = Model(inputs=ae_inputs, outputs=ae_outputs, name='Attention_AE')\n",
    "\n",
    "    return encoder_ae, decoder_ae, ae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab95b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_ae, decoder_ae, ae = make_AE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f776d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d0c31ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strat = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strat.scope():\n",
    "    encoder_ae, decoder_ae, ae = make_AE()\n",
    "    ae.compile(loss =  'mse', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4ece6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.time()\n",
    "datetime_obj = datetime.fromtimestamp(timestamp)\n",
    "fmt_time = datetime_obj.strftime('%m-%d %H:%M:%S')\n",
    "\n",
    "tb = TensorBoard(log_dir=f'vae_logs/{fmt_time}')\n",
    "\n",
    "es = EarlyStopping(monitor = 'val_loss', patience = 15, restore_best_weights=True)\n",
    "\n",
    "ckpt = ModelCheckpoint(filepath = 'vae_cpkts/model-{epoch:02d}-{val_loss:.4f}.hdf5',\n",
    "                      monitor = 'val_loss',\n",
    "                      mode = 'min',\n",
    "                      save_best_only = True,\n",
    "                      verbose = 1)\n",
    "\n",
    "red_lr = ReduceLROnPlateau(patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92e720da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:29:21.238623: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1415\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:4\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2023-07-24 04:29:21.358430: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9878"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:34:47.093381: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_103521\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:32\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2023-07-24 04:34:47.152373: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.98015, saving model to vae_cpkts/model-01-0.9802.hdf5\n",
      "24722/24722 [==============================] - 360s 14ms/step - loss: 0.9878 - val_loss: 0.9802 - lr: 0.0010\n",
      "Epoch 2/10000\n",
      "24719/24722 [============================>.] - ETA: 0s - loss: 0.9870"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:40:40.578282: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.98015 to 0.97980, saving model to vae_cpkts/model-02-0.9798.hdf5\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9870 - val_loss: 0.9798 - lr: 0.0010\n",
      "Epoch 3/10000\n",
      "24720/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:46:33.175701: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss did not improve from 0.97980\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9868 - val_loss: 0.9798 - lr: 0.0010\n",
      "Epoch 4/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:52:26.042157: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.97980 to 0.97967, saving model to vae_cpkts/model-04-0.9797.hdf5\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 5/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 04:58:18.410601: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.97967 to 0.97965, saving model to vae_cpkts/model-05-0.9797.hdf5\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 6/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:04:11.228270: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: val_loss did not improve from 0.97965\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 7/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:10:04.137678: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: val_loss improved from 0.97965 to 0.97965, saving model to vae_cpkts/model-07-0.9797.hdf5\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 8/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:15:56.585162: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: val_loss did not improve from 0.97965\n",
      "24722/24722 [==============================] - 352s 14ms/step - loss: 0.9868 - val_loss: 0.9797 - lr: 0.0010\n",
      "Epoch 9/10000\n",
      "24721/24722 [============================>.] - ETA: 0s - loss: 0.9867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:21:49.378092: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: val_loss improved from 0.97965 to 0.97961, saving model to vae_cpkts/model-09-0.9796.hdf5\n",
      "24722/24722 [==============================] - 353s 14ms/step - loss: 0.9867 - val_loss: 0.9796 - lr: 0.0010\n",
      "Epoch 10/10000\n",
      "24720/24722 [============================>.] - ETA: 0s - loss: 0.9867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 05:27:43.166422: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: val_loss did not improve from 0.97961\n",
      "24722/24722 [==============================] - 354s 14ms/step - loss: 0.9867 - val_loss: 0.9796 - lr: 0.0010\n",
      "Epoch 11/10000\n",
      "21081/24722 [========================>.....] - ETA: 47s - loss: 0.9868"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m*\u001b[39m strat\u001b[38;5;241m.\u001b[39mnum_replicas_in_sync\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val_seq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mred_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m         \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256 * strat.num_replicas_in_sync\n",
    "\n",
    "history = ae.fit(X_train_seq, X_train_seq, validation_data = (X_val_seq, X_val_seq), callbacks = [tb, es, ckpt, red_lr], \n",
    "         epochs = 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
